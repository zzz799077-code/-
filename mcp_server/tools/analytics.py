"""
é«˜çº§æ•°æ®åˆ†æå·¥å…·

æä¾›çƒ­åº¦è¶‹åŠ¿åˆ†æã€å¹³å°å¯¹æ¯”ã€å…³é”®è¯å…±ç°ã€æƒ…æ„Ÿåˆ†æç­‰é«˜çº§åˆ†æåŠŸèƒ½ã€‚
"""

import os
import re
from collections import Counter, defaultdict
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Union
from difflib import SequenceMatcher

import yaml

from trendradar.core.analyzer import calculate_news_weight as _calculate_news_weight

from ..services.data_service import DataService
from ..utils.validators import (
    validate_platforms,
    validate_limit,
    validate_keyword,
    validate_top_n,
    validate_date_range,
    validate_threshold
)
from ..utils.errors import MCPError, InvalidParameterError, DataNotFoundError


def _get_weight_config() -> Dict:
    """
    ä» config.yaml è¯»å–æƒé‡é…ç½®

    Returns:
        æƒé‡é…ç½®å­—å…¸ï¼ŒåŒ…å« RANK_WEIGHT, FREQUENCY_WEIGHT, HOTNESS_WEIGHT
    """
    # é»˜è®¤å€¼
    default_config = {
        "RANK_WEIGHT": 0.6,
        "FREQUENCY_WEIGHT": 0.3,
        "HOTNESS_WEIGHT": 0.1,
    }

    try:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(current_dir, "..", "..", "config", "config.yaml")
        config_path = os.path.normpath(config_path)

        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
            weight = config.get('advanced', {}).get('weight', {})
            return {
                "RANK_WEIGHT": weight.get('rank', 0.6),
                "FREQUENCY_WEIGHT": weight.get('frequency', 0.3),
                "HOTNESS_WEIGHT": weight.get('hotness', 0.1),
            }
    except Exception:
        return default_config


def calculate_news_weight(news_data: Dict, rank_threshold: int = 5) -> float:
    """
    è®¡ç®—æ–°é—»æƒé‡ï¼ˆç”¨äºæ’åºï¼‰

    å¤ç”¨ trendradar.core.analyzer.calculate_news_weight å®ç°ï¼Œ
    æƒé‡é…ç½®ä» config.yaml çš„ advanced.weight è¯»å–ã€‚

    Args:
        news_data: æ–°é—»æ•°æ®å­—å…¸ï¼ŒåŒ…å« ranks å’Œ count å­—æ®µ
        rank_threshold: é«˜æ’åé˜ˆå€¼ï¼Œé»˜è®¤5

    Returns:
        æƒé‡åˆ†æ•°ï¼ˆ0-100ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼‰
    """
    return _calculate_news_weight(news_data, rank_threshold, _get_weight_config())


class AnalyticsTools:
    """é«˜çº§æ•°æ®åˆ†æå·¥å…·ç±»"""

    def __init__(self, project_root: str = None):
        """
        åˆå§‹åŒ–åˆ†æå·¥å…·

        Args:
            project_root: é¡¹ç›®æ ¹ç›®å½•
        """
        self.data_service = DataService(project_root)

    def analyze_data_insights_unified(
        self,
        insight_type: str = "platform_compare",
        topic: Optional[str] = None,
        date_range: Optional[Union[Dict[str, str], str]] = None,
        min_frequency: int = 3,
        top_n: int = 20
    ) -> Dict:
        """
        ç»Ÿä¸€æ•°æ®æ´å¯Ÿåˆ†æå·¥å…· - æ•´åˆå¤šç§æ•°æ®åˆ†ææ¨¡å¼

        Args:
            insight_type: æ´å¯Ÿç±»å‹ï¼Œå¯é€‰å€¼ï¼š
                - "platform_compare": å¹³å°å¯¹æ¯”åˆ†æï¼ˆå¯¹æ¯”ä¸åŒå¹³å°å¯¹è¯é¢˜çš„å…³æ³¨åº¦ï¼‰
                - "platform_activity": å¹³å°æ´»è·ƒåº¦ç»Ÿè®¡ï¼ˆç»Ÿè®¡å„å¹³å°å‘å¸ƒé¢‘ç‡å’Œæ´»è·ƒæ—¶é—´ï¼‰
                - "keyword_cooccur": å…³é”®è¯å…±ç°åˆ†æï¼ˆåˆ†æå…³é”®è¯åŒæ—¶å‡ºç°çš„æ¨¡å¼ï¼‰
            topic: è¯é¢˜å…³é”®è¯ï¼ˆå¯é€‰ï¼Œplatform_compareæ¨¡å¼é€‚ç”¨ï¼‰
            date_range: æ—¥æœŸèŒƒå›´ï¼Œæ ¼å¼: {"start": "YYYY-MM-DD", "end": "YYYY-MM-DD"}
            min_frequency: æœ€å°å…±ç°é¢‘æ¬¡ï¼ˆkeyword_cooccuræ¨¡å¼ï¼‰ï¼Œé»˜è®¤3
            top_n: è¿”å›TOP Nç»“æœï¼ˆkeyword_cooccuræ¨¡å¼ï¼‰ï¼Œé»˜è®¤20

        Returns:
            æ•°æ®æ´å¯Ÿåˆ†æç»“æœå­—å…¸

        Examples:
            - analyze_data_insights_unified(insight_type="platform_compare", topic="äººå·¥æ™ºèƒ½")
            - analyze_data_insights_unified(insight_type="platform_activity", date_range={...})
            - analyze_data_insights_unified(insight_type="keyword_cooccur", min_frequency=5)
        """
        try:
            # å‚æ•°éªŒè¯
            if insight_type not in ["platform_compare", "platform_activity", "keyword_cooccur"]:
                raise InvalidParameterError(
                    f"æ— æ•ˆçš„æ´å¯Ÿç±»å‹: {insight_type}",
                    suggestion="æ”¯æŒçš„ç±»å‹: platform_compare, platform_activity, keyword_cooccur"
                )

            # æ ¹æ®æ´å¯Ÿç±»å‹è°ƒç”¨ç›¸åº”æ–¹æ³•
            if insight_type == "platform_compare":
                return self.compare_platforms(
                    topic=topic,
                    date_range=date_range
                )
            elif insight_type == "platform_activity":
                return self.get_platform_activity_stats(
                    date_range=date_range
                )
            else:  # keyword_cooccur
                return self.analyze_keyword_cooccurrence(
                    min_frequency=min_frequency,
                    top_n=top_n
                )

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def analyze_topic_trend_unified(
        self,
        topic: str,
        analysis_type: str = "trend",
        date_range: Optional[Union[Dict[str, str], str]] = None,
        granularity: str = "day",
        threshold: float = 3.0,
        time_window: int = 24,
        lookahead_hours: int = 6,
        confidence_threshold: float = 0.7
    ) -> Dict:
        """
        ç»Ÿä¸€è¯é¢˜è¶‹åŠ¿åˆ†æå·¥å…· - æ•´åˆå¤šç§è¶‹åŠ¿åˆ†ææ¨¡å¼

        Args:
            topic: è¯é¢˜å…³é”®è¯ï¼ˆå¿…éœ€ï¼‰
            analysis_type: åˆ†æç±»å‹ï¼Œå¯é€‰å€¼ï¼š
                - "trend": çƒ­åº¦è¶‹åŠ¿åˆ†æï¼ˆè¿½è¸ªè¯é¢˜çš„çƒ­åº¦å˜åŒ–ï¼‰
                - "lifecycle": ç”Ÿå‘½å‘¨æœŸåˆ†æï¼ˆä»å‡ºç°åˆ°æ¶ˆå¤±çš„å®Œæ•´å‘¨æœŸï¼‰
                - "viral": å¼‚å¸¸çƒ­åº¦æ£€æµ‹ï¼ˆè¯†åˆ«çªç„¶çˆ†ç«çš„è¯é¢˜ï¼‰
                - "predict": è¯é¢˜é¢„æµ‹ï¼ˆé¢„æµ‹æœªæ¥å¯èƒ½çš„çƒ­ç‚¹ï¼‰
            date_range: æ—¥æœŸèŒƒå›´ï¼ˆtrendå’Œlifecycleæ¨¡å¼ï¼‰ï¼Œå¯é€‰
                       - **æ ¼å¼**: {"start": "YYYY-MM-DD", "end": "YYYY-MM-DD"}
                       - **é»˜è®¤**: ä¸æŒ‡å®šæ—¶é»˜è®¤åˆ†ææœ€è¿‘7å¤©
            granularity: æ—¶é—´ç²’åº¦ï¼ˆtrendæ¨¡å¼ï¼‰ï¼Œé»˜è®¤"day"ï¼ˆhour/dayï¼‰
            threshold: çƒ­åº¦çªå¢å€æ•°é˜ˆå€¼ï¼ˆviralæ¨¡å¼ï¼‰ï¼Œé»˜è®¤3.0
            time_window: æ£€æµ‹æ—¶é—´çª—å£å°æ—¶æ•°ï¼ˆviralæ¨¡å¼ï¼‰ï¼Œé»˜è®¤24
            lookahead_hours: é¢„æµ‹æœªæ¥å°æ—¶æ•°ï¼ˆpredictæ¨¡å¼ï¼‰ï¼Œé»˜è®¤6
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆpredictæ¨¡å¼ï¼‰ï¼Œé»˜è®¤0.7

        Returns:
            è¶‹åŠ¿åˆ†æç»“æœå­—å…¸

        Examples (å‡è®¾ä»Šå¤©æ˜¯ 2025-11-17):
            - ç”¨æˆ·ï¼š"åˆ†æAIæœ€è¿‘7å¤©çš„è¶‹åŠ¿" â†’ analyze_topic_trend_unified(topic="äººå·¥æ™ºèƒ½", analysis_type="trend", date_range={"start": "2025-11-11", "end": "2025-11-17"})
            - ç”¨æˆ·ï¼š"çœ‹çœ‹ç‰¹æ–¯æ‹‰æœ¬æœˆçš„çƒ­åº¦" â†’ analyze_topic_trend_unified(topic="ç‰¹æ–¯æ‹‰", analysis_type="lifecycle", date_range={"start": "2025-11-01", "end": "2025-11-17"})
            - analyze_topic_trend_unified(topic="æ¯”ç‰¹å¸", analysis_type="viral", threshold=3.0)
            - analyze_topic_trend_unified(topic="ChatGPT", analysis_type="predict", lookahead_hours=6)
        """
        try:
            # å‚æ•°éªŒè¯
            topic = validate_keyword(topic)

            if analysis_type not in ["trend", "lifecycle", "viral", "predict"]:
                raise InvalidParameterError(
                    f"æ— æ•ˆçš„åˆ†æç±»å‹: {analysis_type}",
                    suggestion="æ”¯æŒçš„ç±»å‹: trend, lifecycle, viral, predict"
                )

            # æ ¹æ®åˆ†æç±»å‹è°ƒç”¨ç›¸åº”æ–¹æ³•
            if analysis_type == "trend":
                return self.get_topic_trend_analysis(
                    topic=topic,
                    date_range=date_range,
                    granularity=granularity
                )
            elif analysis_type == "lifecycle":
                return self.analyze_topic_lifecycle(
                    topic=topic,
                    date_range=date_range
                )
            elif analysis_type == "viral":
                # viralæ¨¡å¼ä¸éœ€è¦topicå‚æ•°ï¼Œä½¿ç”¨é€šç”¨æ£€æµ‹
                return self.detect_viral_topics(
                    threshold=threshold,
                    time_window=time_window
                )
            else:  # predict
                # predictæ¨¡å¼ä¸éœ€è¦topicå‚æ•°ï¼Œä½¿ç”¨é€šç”¨é¢„æµ‹
                return self.predict_trending_topics(
                    lookahead_hours=lookahead_hours,
                    confidence_threshold=confidence_threshold
                )

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def get_topic_trend_analysis(
        self,
        topic: str,
        date_range: Optional[Union[Dict[str, str], str]] = None,
        granularity: str = "day"
    ) -> Dict:
        """
        çƒ­åº¦è¶‹åŠ¿åˆ†æ - è¿½è¸ªç‰¹å®šè¯é¢˜çš„çƒ­åº¦å˜åŒ–è¶‹åŠ¿

        Args:
            topic: è¯é¢˜å…³é”®è¯
            date_range: æ—¥æœŸèŒƒå›´ï¼ˆå¯é€‰ï¼‰
                       - **æ ¼å¼**: {"start": "YYYY-MM-DD", "end": "YYYY-MM-DD"}
                       - **é»˜è®¤**: ä¸æŒ‡å®šæ—¶é»˜è®¤åˆ†ææœ€è¿‘7å¤©
            granularity: æ—¶é—´ç²’åº¦ï¼Œä»…æ”¯æŒ dayï¼ˆå¤©ï¼‰

        Returns:
            è¶‹åŠ¿åˆ†æç»“æœå­—å…¸

        Examples:
            ç”¨æˆ·è¯¢é—®ç¤ºä¾‹ï¼š
            - "å¸®æˆ‘åˆ†æä¸€ä¸‹'äººå·¥æ™ºèƒ½'è¿™ä¸ªè¯é¢˜æœ€è¿‘ä¸€å‘¨çš„çƒ­åº¦è¶‹åŠ¿"
            - "æŸ¥çœ‹'æ¯”ç‰¹å¸'è¿‡å»ä¸€å‘¨çš„çƒ­åº¦å˜åŒ–"
            - "çœ‹çœ‹'iPhone'æœ€è¿‘7å¤©çš„è¶‹åŠ¿å¦‚ä½•"
            - "åˆ†æ'ç‰¹æ–¯æ‹‰'æœ€è¿‘ä¸€ä¸ªæœˆçš„çƒ­åº¦è¶‹åŠ¿"
            - "æŸ¥çœ‹'ChatGPT'2024å¹´12æœˆçš„è¶‹åŠ¿å˜åŒ–"

            ä»£ç è°ƒç”¨ç¤ºä¾‹ï¼š
            >>> tools = AnalyticsTools()
            >>> # åˆ†æ7å¤©è¶‹åŠ¿ï¼ˆå‡è®¾ä»Šå¤©æ˜¯ 2025-11-17ï¼‰
            >>> result = tools.get_topic_trend_analysis(
            ...     topic="äººå·¥æ™ºèƒ½",
            ...     date_range={"start": "2025-11-11", "end": "2025-11-17"},
            ...     granularity="day"
            ... )
            >>> # åˆ†æå†å²æœˆä»½è¶‹åŠ¿
            >>> result = tools.get_topic_trend_analysis(
            ...     topic="ç‰¹æ–¯æ‹‰",
            ...     date_range={"start": "2024-12-01", "end": "2024-12-31"},
            ...     granularity="day"
            ... )
            >>> print(result['trend_data'])
        """
        try:
            # éªŒè¯å‚æ•°
            topic = validate_keyword(topic)

            # éªŒè¯ç²’åº¦å‚æ•°ï¼ˆåªæ”¯æŒdayï¼‰
            if granularity != "day":
                from ..utils.errors import InvalidParameterError
                raise InvalidParameterError(
                    f"ä¸æ”¯æŒçš„ç²’åº¦å‚æ•°: {granularity}",
                    suggestion="å½“å‰ä»…æ”¯æŒ 'day' ç²’åº¦ï¼Œå› ä¸ºåº•å±‚æ•°æ®æŒ‰å¤©èšåˆ"
                )

            # å¤„ç†æ—¥æœŸèŒƒå›´ï¼ˆä¸æŒ‡å®šæ—¶é»˜è®¤æœ€è¿‘7å¤©ï¼‰
            if date_range:
                from ..utils.validators import validate_date_range
                date_range_tuple = validate_date_range(date_range)
                start_date, end_date = date_range_tuple
            else:
                # é»˜è®¤æœ€è¿‘7å¤©
                end_date = datetime.now()
                start_date = end_date - timedelta(days=6)

            # æ”¶é›†è¶‹åŠ¿æ•°æ®
            trend_data = []
            current_date = start_date

            while current_date <= end_date:
                try:
                    all_titles, _, _ = self.data_service.parser.read_all_titles_for_date(
                        date=current_date
                    )

                    # ç»Ÿè®¡è¯¥æ—¶é—´ç‚¹çš„è¯é¢˜å‡ºç°æ¬¡æ•°
                    count = 0
                    matched_titles = []

                    for _, titles in all_titles.items():
                        for title in titles.keys():
                            if topic.lower() in title.lower():
                                count += 1
                                matched_titles.append(title)

                    trend_data.append({
                        "date": current_date.strftime("%Y-%m-%d"),
                        "count": count,
                        "sample_titles": matched_titles[:3]  # åªä¿ç•™å‰3ä¸ªæ ·æœ¬
                    })

                except DataNotFoundError:
                    trend_data.append({
                        "date": current_date.strftime("%Y-%m-%d"),
                        "count": 0,
                        "sample_titles": []
                    })

                # æŒ‰å¤©å¢åŠ æ—¶é—´
                current_date += timedelta(days=1)

            # è®¡ç®—è¶‹åŠ¿æŒ‡æ ‡
            counts = [item["count"] for item in trend_data]
            total_days = (end_date - start_date).days + 1

            if len(counts) >= 2:
                # è®¡ç®—æ¶¨è·Œå¹…åº¦
                first_non_zero = next((c for c in counts if c > 0), 0)
                last_count = counts[-1]

                if first_non_zero > 0:
                    change_rate = ((last_count - first_non_zero) / first_non_zero) * 100
                else:
                    change_rate = 0

                # æ‰¾åˆ°å³°å€¼æ—¶é—´
                max_count = max(counts)
                peak_index = counts.index(max_count)
                peak_time = trend_data[peak_index]["date"]
            else:
                change_rate = 0
                peak_time = None
                max_count = 0

            return {
                "success": True,
                "summary": {
                    "description": f"è¯é¢˜ã€Œ{topic}ã€çš„çƒ­åº¦è¶‹åŠ¿åˆ†æ",
                    "topic": topic,
                    "date_range": {
                        "start": start_date.strftime("%Y-%m-%d"),
                        "end": end_date.strftime("%Y-%m-%d"),
                        "total_days": total_days
                    },
                    "granularity": granularity,
                    "total_mentions": sum(counts),
                    "average_mentions": round(sum(counts) / len(counts), 2) if counts else 0,
                    "peak_count": max_count,
                    "peak_time": peak_time,
                    "change_rate": round(change_rate, 2),
                    "trend_direction": "ä¸Šå‡" if change_rate > 10 else "ä¸‹é™" if change_rate < -10 else "ç¨³å®š"
                },
                "data": trend_data
            }

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def compare_platforms(
        self,
        topic: Optional[str] = None,
        date_range: Optional[Union[Dict[str, str], str]] = None
    ) -> Dict:
        """
        å¹³å°å¯¹æ¯”åˆ†æ - å¯¹æ¯”ä¸åŒå¹³å°å¯¹åŒä¸€è¯é¢˜çš„å…³æ³¨åº¦

        Args:
            topic: è¯é¢˜å…³é”®è¯ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™å¯¹æ¯”æ•´ä½“æ´»è·ƒåº¦ï¼‰
            date_range: æ—¥æœŸèŒƒå›´ï¼Œæ ¼å¼: {"start": "YYYY-MM-DD", "end": "YYYY-MM-DD"}

        Returns:
            å¹³å°å¯¹æ¯”åˆ†æç»“æœ

        Examples:
            ç”¨æˆ·è¯¢é—®ç¤ºä¾‹ï¼š
            - "å¯¹æ¯”ä¸€ä¸‹å„ä¸ªå¹³å°å¯¹'äººå·¥æ™ºèƒ½'è¯é¢˜çš„å…³æ³¨åº¦"
            - "çœ‹çœ‹çŸ¥ä¹å’Œå¾®åšå“ªä¸ªå¹³å°æ›´å…³æ³¨ç§‘æŠ€æ–°é—»"
            - "åˆ†æå„å¹³å°ä»Šå¤©çš„çƒ­ç‚¹åˆ†å¸ƒ"

            ä»£ç è°ƒç”¨ç¤ºä¾‹ï¼š
            >>> # å¯¹æ¯”å„å¹³å°ï¼ˆå‡è®¾ä»Šå¤©æ˜¯ 2025-11-17ï¼‰
            >>> result = tools.compare_platforms(
            ...     topic="äººå·¥æ™ºèƒ½",
            ...     date_range={"start": "2025-11-08", "end": "2025-11-17"}
            ... )
            >>> print(result['platform_stats'])
        """
        try:
            # å‚æ•°éªŒè¯
            if topic:
                topic = validate_keyword(topic)
            date_range_tuple = validate_date_range(date_range)

            # ç¡®å®šæ—¥æœŸèŒƒå›´
            if date_range_tuple:
                start_date, end_date = date_range_tuple
            else:
                start_date = end_date = datetime.now()

            # æ”¶é›†å„å¹³å°æ•°æ®
            platform_stats = defaultdict(lambda: {
                "total_news": 0,
                "topic_mentions": 0,
                "unique_titles": set(),
                "top_keywords": Counter()
            })

            # éå†æ—¥æœŸèŒƒå›´
            current_date = start_date
            while current_date <= end_date:
                try:
                    all_titles, id_to_name, _ = self.data_service.parser.read_all_titles_for_date(
                        date=current_date
                    )

                    for platform_id, titles in all_titles.items():
                        platform_name = id_to_name.get(platform_id, platform_id)

                        for title in titles.keys():
                            platform_stats[platform_name]["total_news"] += 1
                            platform_stats[platform_name]["unique_titles"].add(title)

                            # å¦‚æœæŒ‡å®šäº†è¯é¢˜ï¼Œç»Ÿè®¡åŒ…å«è¯é¢˜çš„æ–°é—»
                            if topic and topic.lower() in title.lower():
                                platform_stats[platform_name]["topic_mentions"] += 1

                            # æå–å…³é”®è¯ï¼ˆç®€å•åˆ†è¯ï¼‰
                            keywords = self._extract_keywords(title)
                            platform_stats[platform_name]["top_keywords"].update(keywords)

                except DataNotFoundError:
                    pass

                current_date += timedelta(days=1)

            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            result_stats = {}
            for platform, stats in platform_stats.items():
                coverage_rate = 0
                if stats["total_news"] > 0:
                    coverage_rate = (stats["topic_mentions"] / stats["total_news"]) * 100

                result_stats[platform] = {
                    "total_news": stats["total_news"],
                    "topic_mentions": stats["topic_mentions"],
                    "unique_titles": len(stats["unique_titles"]),
                    "coverage_rate": round(coverage_rate, 2),
                    "top_keywords": [
                        {"keyword": k, "count": v}
                        for k, v in stats["top_keywords"].most_common(5)
                    ]
                }

            # æ‰¾å‡ºå„å¹³å°ç‹¬æœ‰çš„çƒ­ç‚¹
            unique_topics = self._find_unique_topics(platform_stats)

            return {
                "success": True,
                "topic": topic,
                "date_range": {
                    "start": start_date.strftime("%Y-%m-%d"),
                    "end": end_date.strftime("%Y-%m-%d")
                },
                "platform_stats": result_stats,
                "unique_topics": unique_topics,
                "total_platforms": len(result_stats)
            }

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def analyze_keyword_cooccurrence(
        self,
        min_frequency: int = 3,
        top_n: int = 20
    ) -> Dict:
        """
        å…³é”®è¯å…±ç°åˆ†æ - åˆ†æå“ªäº›å…³é”®è¯ç»å¸¸åŒæ—¶å‡ºç°

        Args:
            min_frequency: æœ€å°å…±ç°é¢‘æ¬¡
            top_n: è¿”å›TOP Nå…³é”®è¯å¯¹

        Returns:
            å…³é”®è¯å…±ç°åˆ†æç»“æœ

        Examples:
            ç”¨æˆ·è¯¢é—®ç¤ºä¾‹ï¼š
            - "åˆ†æä¸€ä¸‹å“ªäº›å…³é”®è¯ç»å¸¸ä¸€èµ·å‡ºç°"
            - "çœ‹çœ‹'äººå·¥æ™ºèƒ½'ç»å¸¸å’Œå“ªäº›è¯ä¸€èµ·å‡ºç°"
            - "æ‰¾å‡ºä»Šå¤©æ–°é—»ä¸­çš„å…³é”®è¯å…³è”"

            ä»£ç è°ƒç”¨ç¤ºä¾‹ï¼š
            >>> tools = AnalyticsTools()
            >>> result = tools.analyze_keyword_cooccurrence(
            ...     min_frequency=5,
            ...     top_n=15
            ... )
            >>> print(result['cooccurrence_pairs'])
        """
        try:
            # å‚æ•°éªŒè¯
            min_frequency = validate_limit(min_frequency, default=3, max_limit=100)
            top_n = validate_top_n(top_n, default=20)

            # è¯»å–ä»Šå¤©çš„æ•°æ®
            all_titles, _, _ = self.data_service.parser.read_all_titles_for_date()

            # å…³é”®è¯å…±ç°ç»Ÿè®¡
            cooccurrence = Counter()
            keyword_titles = defaultdict(list)

            for platform_id, titles in all_titles.items():
                for title in titles.keys():
                    # æå–å…³é”®è¯
                    keywords = self._extract_keywords(title)

                    # è®°å½•æ¯ä¸ªå…³é”®è¯å‡ºç°çš„æ ‡é¢˜
                    for kw in keywords:
                        keyword_titles[kw].append(title)

                    # è®¡ç®—ä¸¤ä¸¤å…±ç°
                    if len(keywords) >= 2:
                        for i, kw1 in enumerate(keywords):
                            for kw2 in keywords[i+1:]:
                                # ç»Ÿä¸€æ’åºï¼Œé¿å…é‡å¤
                                pair = tuple(sorted([kw1, kw2]))
                                cooccurrence[pair] += 1

            # è¿‡æ»¤ä½é¢‘å…±ç°
            filtered_pairs = [
                (pair, count) for pair, count in cooccurrence.items()
                if count >= min_frequency
            ]

            # æ’åºå¹¶å–TOP N
            top_pairs = sorted(filtered_pairs, key=lambda x: x[1], reverse=True)[:top_n]

            # æ„å»ºç»“æœ
            result_pairs = []
            for (kw1, kw2), count in top_pairs:
                # æ‰¾å‡ºåŒæ—¶åŒ…å«ä¸¤ä¸ªå…³é”®è¯çš„æ ‡é¢˜æ ·æœ¬
                titles_with_both = [
                    title for title in keyword_titles[kw1]
                    if kw2 in self._extract_keywords(title)
                ]

                result_pairs.append({
                    "keyword1": kw1,
                    "keyword2": kw2,
                    "cooccurrence_count": count,
                    "sample_titles": titles_with_both[:3]
                })

            return {
                "success": True,
                "summary": {
                    "description": "å…³é”®è¯å…±ç°åˆ†æç»“æœ",
                    "total": len(result_pairs),
                    "min_frequency": min_frequency,
                    "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                },
                "data": result_pairs
            }

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def analyze_sentiment(
        self,
        topic: Optional[str] = None,
        platforms: Optional[List[str]] = None,
        date_range: Optional[Union[Dict[str, str], str]] = None,
        limit: int = 50,
        sort_by_weight: bool = True,
        include_url: bool = False
    ) -> Dict:
        """
        æƒ…æ„Ÿå€¾å‘åˆ†æ - ç”Ÿæˆç”¨äº AI æƒ…æ„Ÿåˆ†æçš„ç»“æ„åŒ–æç¤ºè¯

        æœ¬å·¥å…·æ”¶é›†æ–°é—»æ•°æ®å¹¶ç”Ÿæˆä¼˜åŒ–çš„ AI æç¤ºè¯ï¼Œä½ å¯ä»¥å°†å…¶å‘é€ç»™ AI è¿›è¡Œæ·±åº¦æƒ…æ„Ÿåˆ†æã€‚

        Args:
            topic: è¯é¢˜å…³é”®è¯ï¼ˆå¯é€‰ï¼‰ï¼Œåªåˆ†æåŒ…å«è¯¥å…³é”®è¯çš„æ–°é—»
            platforms: å¹³å°è¿‡æ»¤åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚ ['zhihu', 'weibo']
            date_range: æ—¥æœŸèŒƒå›´ï¼ˆå¯é€‰ï¼‰ï¼Œæ ¼å¼: {"start": "YYYY-MM-DD", "end": "YYYY-MM-DD"}
                       ä¸æŒ‡å®šåˆ™é»˜è®¤æŸ¥è¯¢ä»Šå¤©çš„æ•°æ®
            limit: è¿”å›æ–°é—»æ•°é‡é™åˆ¶ï¼Œé»˜è®¤50ï¼Œæœ€å¤§100
            sort_by_weight: æ˜¯å¦æŒ‰æƒé‡æ’åºï¼Œé»˜è®¤Trueï¼ˆæ¨èï¼‰
            include_url: æ˜¯å¦åŒ…å«URLé“¾æ¥ï¼Œé»˜è®¤Falseï¼ˆèŠ‚çœtokenï¼‰

        Returns:
            åŒ…å« AI æç¤ºè¯å’Œæ–°é—»æ•°æ®çš„ç»“æ„åŒ–ç»“æœ

        Examples:
            ç”¨æˆ·è¯¢é—®ç¤ºä¾‹ï¼š
            - "åˆ†æä¸€ä¸‹ä»Šå¤©æ–°é—»çš„æƒ…æ„Ÿå€¾å‘"
            - "çœ‹çœ‹'ç‰¹æ–¯æ‹‰'ç›¸å…³æ–°é—»æ˜¯æ­£é¢è¿˜æ˜¯è´Ÿé¢çš„"
            - "åˆ†æå„å¹³å°å¯¹'äººå·¥æ™ºèƒ½'çš„æƒ…æ„Ÿæ€åº¦"
            - "çœ‹çœ‹'ç‰¹æ–¯æ‹‰'ç›¸å…³æ–°é—»æ˜¯æ­£é¢è¿˜æ˜¯è´Ÿé¢çš„ï¼Œè¯·é€‰æ‹©ä¸€å‘¨å†…çš„å‰10æ¡æ–°é—»æ¥åˆ†æ"

            ä»£ç è°ƒç”¨ç¤ºä¾‹ï¼š
            >>> tools = AnalyticsTools()
            >>> # åˆ†æä»Šå¤©çš„ç‰¹æ–¯æ‹‰æ–°é—»ï¼Œè¿”å›å‰10æ¡
            >>> result = tools.analyze_sentiment(
            ...     topic="ç‰¹æ–¯æ‹‰",
            ...     limit=10
            ... )
            >>> # åˆ†æä¸€å‘¨å†…çš„ç‰¹æ–¯æ‹‰æ–°é—»ï¼ˆå‡è®¾ä»Šå¤©æ˜¯ 2025-11-17ï¼‰
            >>> result = tools.analyze_sentiment(
            ...     topic="ç‰¹æ–¯æ‹‰",
            ...     date_range={"start": "2025-11-11", "end": "2025-11-17"},
            ...     limit=10
            ... )
            >>> print(result['ai_prompt'])  # è·å–ç”Ÿæˆçš„æç¤ºè¯
        """
        try:
            # å‚æ•°éªŒè¯
            if topic:
                topic = validate_keyword(topic)
            platforms = validate_platforms(platforms)
            limit = validate_limit(limit, default=50)

            # å¤„ç†æ—¥æœŸèŒƒå›´
            if date_range:
                date_range_tuple = validate_date_range(date_range)
                start_date, end_date = date_range_tuple
            else:
                # é»˜è®¤ä»Šå¤©
                start_date = end_date = datetime.now()

            # æ”¶é›†æ–°é—»æ•°æ®ï¼ˆæ”¯æŒå¤šå¤©ï¼‰
            all_news_items = []
            current_date = start_date

            while current_date <= end_date:
                try:
                    all_titles, id_to_name, _ = self.data_service.parser.read_all_titles_for_date(
                        date=current_date,
                        platform_ids=platforms
                    )

                    # æ”¶é›†è¯¥æ—¥æœŸçš„æ–°é—»
                    for platform_id, titles in all_titles.items():
                        platform_name = id_to_name.get(platform_id, platform_id)
                        for title, info in titles.items():
                            # å¦‚æœæŒ‡å®šäº†è¯é¢˜ï¼Œåªæ”¶é›†åŒ…å«è¯é¢˜çš„æ ‡é¢˜
                            if topic and topic.lower() not in title.lower():
                                continue

                            news_item = {
                                "platform": platform_name,
                                "title": title,
                                "ranks": info.get("ranks", []),
                                "count": len(info.get("ranks", [])),
                                "date": current_date.strftime("%Y-%m-%d")
                            }

                            # æ¡ä»¶æ€§æ·»åŠ  URL å­—æ®µ
                            if include_url:
                                news_item["url"] = info.get("url", "")
                                news_item["mobileUrl"] = info.get("mobileUrl", "")

                            all_news_items.append(news_item)

                except DataNotFoundError:
                    # è¯¥æ—¥æœŸæ²¡æœ‰æ•°æ®ï¼Œç»§ç»­ä¸‹ä¸€å¤©
                    pass

                # ä¸‹ä¸€å¤©
                current_date += timedelta(days=1)

            if not all_news_items:
                time_desc = "ä»Šå¤©" if start_date == end_date else f"{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}"
                raise DataNotFoundError(
                    f"æœªæ‰¾åˆ°ç›¸å…³æ–°é—»ï¼ˆ{time_desc}ï¼‰",
                    suggestion="è¯·å°è¯•å…¶ä»–è¯é¢˜ã€æ—¥æœŸèŒƒå›´æˆ–å¹³å°"
                )

            # å»é‡ï¼ˆåŒä¸€æ ‡é¢˜åªä¿ç•™ä¸€æ¬¡ï¼‰
            unique_news = {}
            for item in all_news_items:
                key = f"{item['platform']}::{item['title']}"
                if key not in unique_news:
                    unique_news[key] = item
                else:
                    # åˆå¹¶ ranksï¼ˆå¦‚æœåŒä¸€æ–°é—»åœ¨å¤šå¤©å‡ºç°ï¼‰
                    existing = unique_news[key]
                    existing["ranks"].extend(item["ranks"])
                    existing["count"] = len(existing["ranks"])

            deduplicated_news = list(unique_news.values())

            # æŒ‰æƒé‡æ’åºï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if sort_by_weight:
                deduplicated_news.sort(
                    key=lambda x: calculate_news_weight(x),
                    reverse=True
                )

            # é™åˆ¶è¿”å›æ•°é‡
            selected_news = deduplicated_news[:limit]

            # ç”Ÿæˆ AI æç¤ºè¯
            ai_prompt = self._create_sentiment_analysis_prompt(
                news_data=selected_news,
                topic=topic
            )

            # æ„å»ºæ—¶é—´èŒƒå›´æè¿°
            if start_date == end_date:
                time_range_desc = start_date.strftime("%Y-%m-%d")
            else:
                time_range_desc = f"{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}"

            result = {
                "success": True,
                "method": "ai_prompt_generation",
                "summary": {
                    "description": "æƒ…æ„Ÿåˆ†ææ•°æ®å’ŒAIæç¤ºè¯",
                    "total_found": len(deduplicated_news),
                    "returned": len(selected_news),
                    "requested_limit": limit,
                    "duplicates_removed": len(all_news_items) - len(deduplicated_news),
                    "topic": topic,
                    "time_range": time_range_desc,
                    "platforms": list(set(item["platform"] for item in selected_news)),
                    "sorted_by_weight": sort_by_weight
                },
                "ai_prompt": ai_prompt,
                "data": selected_news,
                "usage_note": "è¯·å°† ai_prompt å­—æ®µçš„å†…å®¹å‘é€ç»™ AI è¿›è¡Œæƒ…æ„Ÿåˆ†æ"
            }

            # å¦‚æœè¿”å›æ•°é‡å°‘äºè¯·æ±‚æ•°é‡ï¼Œå¢åŠ æç¤º
            if len(selected_news) < limit and len(deduplicated_news) >= limit:
                result["note"] = "è¿”å›æ•°é‡å°‘äºè¯·æ±‚æ•°é‡æ˜¯å› ä¸ºå»é‡é€»è¾‘ï¼ˆåŒä¸€æ ‡é¢˜åœ¨ä¸åŒå¹³å°åªä¿ç•™ä¸€æ¬¡ï¼‰"
            elif len(deduplicated_news) < limit:
                result["note"] = f"åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…ä»…æ‰¾åˆ° {len(deduplicated_news)} æ¡åŒ¹é…çš„æ–°é—»"

            return result

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def _create_sentiment_analysis_prompt(
        self,
        news_data: List[Dict],
        topic: Optional[str]
    ) -> str:
        """
        åˆ›å»ºæƒ…æ„Ÿåˆ†æçš„ AI æç¤ºè¯

        Args:
            news_data: æ–°é—»æ•°æ®åˆ—è¡¨ï¼ˆå·²æ’åºå’Œé™åˆ¶æ•°é‡ï¼‰
            topic: è¯é¢˜å…³é”®è¯

        Returns:
            æ ¼å¼åŒ–çš„ AI æç¤ºè¯
        """
        # æŒ‰å¹³å°åˆ†ç»„
        platform_news = defaultdict(list)
        for item in news_data:
            platform_news[item["platform"]].append({
                "title": item["title"],
                "date": item.get("date", "")
            })

        # æ„å»ºæç¤ºè¯
        prompt_parts = []

        # 1. ä»»åŠ¡è¯´æ˜
        if topic:
            prompt_parts.append(f"è¯·åˆ†æä»¥ä¸‹å…³äºã€Œ{topic}ã€çš„æ–°é—»æ ‡é¢˜çš„æƒ…æ„Ÿå€¾å‘ã€‚")
        else:
            prompt_parts.append("è¯·åˆ†æä»¥ä¸‹æ–°é—»æ ‡é¢˜çš„æƒ…æ„Ÿå€¾å‘ã€‚")

        prompt_parts.append("")
        prompt_parts.append("åˆ†æè¦æ±‚ï¼š")
        prompt_parts.append("1. è¯†åˆ«æ¯æ¡æ–°é—»çš„æƒ…æ„Ÿå€¾å‘ï¼ˆæ­£é¢/è´Ÿé¢/ä¸­æ€§ï¼‰")
        prompt_parts.append("2. ç»Ÿè®¡å„æƒ…æ„Ÿç±»åˆ«çš„æ•°é‡å’Œç™¾åˆ†æ¯”")
        prompt_parts.append("3. åˆ†æä¸åŒå¹³å°çš„æƒ…æ„Ÿå·®å¼‚")
        prompt_parts.append("4. æ€»ç»“æ•´ä½“æƒ…æ„Ÿè¶‹åŠ¿")
        prompt_parts.append("5. åˆ—ä¸¾å…¸å‹çš„æ­£é¢å’Œè´Ÿé¢æ–°é—»æ ·æœ¬")
        prompt_parts.append("")

        # 2. æ•°æ®æ¦‚è§ˆ
        prompt_parts.append(f"æ•°æ®æ¦‚è§ˆï¼š")
        prompt_parts.append(f"- æ€»æ–°é—»æ•°ï¼š{len(news_data)}")
        prompt_parts.append(f"- è¦†ç›–å¹³å°ï¼š{len(platform_news)}")

        # æ—¶é—´èŒƒå›´
        dates = set(item.get("date", "") for item in news_data if item.get("date"))
        if dates:
            date_list = sorted(dates)
            if len(date_list) == 1:
                prompt_parts.append(f"- æ—¶é—´èŒƒå›´ï¼š{date_list[0]}")
            else:
                prompt_parts.append(f"- æ—¶é—´èŒƒå›´ï¼š{date_list[0]} è‡³ {date_list[-1]}")

        prompt_parts.append("")

        # 3. æŒ‰å¹³å°å±•ç¤ºæ–°é—»
        prompt_parts.append("æ–°é—»åˆ—è¡¨ï¼ˆæŒ‰å¹³å°åˆ†ç±»ï¼Œå·²æŒ‰é‡è¦æ€§æ’åºï¼‰ï¼š")
        prompt_parts.append("")

        for platform, items in sorted(platform_news.items()):
            prompt_parts.append(f"ã€{platform}ã€‘({len(items)} æ¡)")
            for i, item in enumerate(items, 1):
                title = item["title"]
                date_str = f" [{item['date']}]" if item.get("date") else ""
                prompt_parts.append(f"{i}. {title}{date_str}")
            prompt_parts.append("")

        # 4. è¾“å‡ºæ ¼å¼è¯´æ˜
        prompt_parts.append("è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼š")
        prompt_parts.append("")
        prompt_parts.append("## æƒ…æ„Ÿåˆ†å¸ƒç»Ÿè®¡")
        prompt_parts.append("- æ­£é¢ï¼šXXæ¡ (XX%)")
        prompt_parts.append("- è´Ÿé¢ï¼šXXæ¡ (XX%)")
        prompt_parts.append("- ä¸­æ€§ï¼šXXæ¡ (XX%)")
        prompt_parts.append("")
        prompt_parts.append("## å¹³å°æƒ…æ„Ÿå¯¹æ¯”")
        prompt_parts.append("[å„å¹³å°çš„æƒ…æ„Ÿå€¾å‘å·®å¼‚]")
        prompt_parts.append("")
        prompt_parts.append("## æ•´ä½“æƒ…æ„Ÿè¶‹åŠ¿")
        prompt_parts.append("[æ€»ä½“åˆ†æå’Œå…³é”®å‘ç°]")
        prompt_parts.append("")
        prompt_parts.append("## å…¸å‹æ ·æœ¬")
        prompt_parts.append("æ­£é¢æ–°é—»æ ·æœ¬ï¼š")
        prompt_parts.append("[åˆ—ä¸¾3-5æ¡]")
        prompt_parts.append("")
        prompt_parts.append("è´Ÿé¢æ–°é—»æ ·æœ¬ï¼š")
        prompt_parts.append("[åˆ—ä¸¾3-5æ¡]")

        return "\n".join(prompt_parts)

    def find_similar_news(
        self,
        reference_title: str,
        threshold: float = 0.6,
        limit: int = 50,
        include_url: bool = False
    ) -> Dict:
        """
        ç›¸ä¼¼æ–°é—»æŸ¥æ‰¾ - åŸºäºæ ‡é¢˜ç›¸ä¼¼åº¦æŸ¥æ‰¾ç›¸å…³æ–°é—»

        Args:
            reference_title: å‚è€ƒæ ‡é¢˜
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ä¹‹é—´ï¼‰
            limit: è¿”å›æ¡æ•°é™åˆ¶ï¼Œé»˜è®¤50
            include_url: æ˜¯å¦åŒ…å«URLé“¾æ¥ï¼Œé»˜è®¤Falseï¼ˆèŠ‚çœtokenï¼‰

        Returns:
            ç›¸ä¼¼æ–°é—»åˆ—è¡¨

        Examples:
            ç”¨æˆ·è¯¢é—®ç¤ºä¾‹ï¼š
            - "æ‰¾å‡ºå’Œ'ç‰¹æ–¯æ‹‰é™ä»·'ç›¸ä¼¼çš„æ–°é—»"
            - "æŸ¥æ‰¾å…³äºiPhoneå‘å¸ƒçš„ç±»ä¼¼æŠ¥é“"
            - "çœ‹çœ‹æœ‰æ²¡æœ‰å’Œè¿™æ¡æ–°é—»ç›¸ä¼¼çš„æŠ¥é“"

            ä»£ç è°ƒç”¨ç¤ºä¾‹ï¼š
            >>> tools = AnalyticsTools()
            >>> result = tools.find_similar_news(
            ...     reference_title="ç‰¹æ–¯æ‹‰å®£å¸ƒé™ä»·",
            ...     threshold=0.6,
            ...     limit=10
            ... )
            >>> print(result['similar_news'])
        """
        try:
            # å‚æ•°éªŒè¯
            reference_title = validate_keyword(reference_title)
            threshold = validate_threshold(threshold, default=0.6, min_value=0.0, max_value=1.0)
            limit = validate_limit(limit, default=50)

            # è¯»å–æ•°æ®
            all_titles, id_to_name, _ = self.data_service.parser.read_all_titles_for_date()

            # è®¡ç®—ç›¸ä¼¼åº¦
            similar_items = []

            for platform_id, titles in all_titles.items():
                platform_name = id_to_name.get(platform_id, platform_id)

                for title, info in titles.items():
                    if title == reference_title:
                        continue

                    # è®¡ç®—ç›¸ä¼¼åº¦
                    similarity = self._calculate_similarity(reference_title, title)

                    if similarity >= threshold:
                        news_item = {
                            "title": title,
                            "platform": platform_id,
                            "platform_name": platform_name,
                            "similarity": round(similarity, 3),
                            "rank": info["ranks"][0] if info["ranks"] else 0
                        }

                        # æ¡ä»¶æ€§æ·»åŠ  URL å­—æ®µ
                        if include_url:
                            news_item["url"] = info.get("url", "")

                        similar_items.append(news_item)

            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            similar_items.sort(key=lambda x: x["similarity"], reverse=True)

            # é™åˆ¶æ•°é‡
            result_items = similar_items[:limit]

            if not result_items:
                raise DataNotFoundError(
                    f"æœªæ‰¾åˆ°ç›¸ä¼¼åº¦è¶…è¿‡ {threshold} çš„æ–°é—»",
                    suggestion="è¯·é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼æˆ–å°è¯•å…¶ä»–æ ‡é¢˜"
                )

            result = {
                "success": True,
                "summary": {
                    "description": "ç›¸ä¼¼æ–°é—»æœç´¢ç»“æœ",
                    "total_found": len(similar_items),
                    "returned": len(result_items),
                    "requested_limit": limit,
                    "threshold": threshold,
                    "reference_title": reference_title
                },
                "data": result_items
            }

            if len(similar_items) < limit:
                result["note"] = f"ç›¸ä¼¼åº¦é˜ˆå€¼ {threshold} ä¸‹ä»…æ‰¾åˆ° {len(similar_items)} æ¡ç›¸ä¼¼æ–°é—»"

            return result

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def search_by_entity(
        self,
        entity: str,
        entity_type: Optional[str] = None,
        limit: int = 50,
        sort_by_weight: bool = True
    ) -> Dict:
        """
        å®ä½“è¯†åˆ«æœç´¢ - æœç´¢åŒ…å«ç‰¹å®šäººç‰©/åœ°ç‚¹/æœºæ„çš„æ–°é—»

        Args:
            entity: å®ä½“åç§°
            entity_type: å®ä½“ç±»å‹ï¼ˆperson/location/organizationï¼‰ï¼Œå¯é€‰
            limit: è¿”å›æ¡æ•°é™åˆ¶ï¼Œé»˜è®¤50ï¼Œæœ€å¤§200
            sort_by_weight: æ˜¯å¦æŒ‰æƒé‡æ’åºï¼Œé»˜è®¤True

        Returns:
            å®ä½“ç›¸å…³æ–°é—»åˆ—è¡¨

        Examples:
            ç”¨æˆ·è¯¢é—®ç¤ºä¾‹ï¼š
            - "æœç´¢é©¬æ–¯å…‹ç›¸å…³çš„æ–°é—»"
            - "æŸ¥æ‰¾å…³äºç‰¹æ–¯æ‹‰å…¬å¸çš„æŠ¥é“ï¼Œè¿”å›å‰20æ¡"
            - "çœ‹çœ‹åŒ—äº¬æœ‰ä»€ä¹ˆæ–°é—»"

            ä»£ç è°ƒç”¨ç¤ºä¾‹ï¼š
            >>> tools = AnalyticsTools()
            >>> result = tools.search_by_entity(
            ...     entity="é©¬æ–¯å…‹",
            ...     entity_type="person",
            ...     limit=20
            ... )
            >>> print(result['related_news'])
        """
        try:
            # å‚æ•°éªŒè¯
            entity = validate_keyword(entity)
            limit = validate_limit(limit, default=50)

            if entity_type and entity_type not in ["person", "location", "organization"]:
                raise InvalidParameterError(
                    f"æ— æ•ˆçš„å®ä½“ç±»å‹: {entity_type}",
                    suggestion="æ”¯æŒçš„ç±»å‹: person, location, organization"
                )

            # è¯»å–æ•°æ®
            all_titles, id_to_name, _ = self.data_service.parser.read_all_titles_for_date()

            # æœç´¢åŒ…å«å®ä½“çš„æ–°é—»
            related_news = []
            entity_context = Counter()  # ç»Ÿè®¡å®ä½“å‘¨è¾¹çš„è¯

            for platform_id, titles in all_titles.items():
                platform_name = id_to_name.get(platform_id, platform_id)

                for title, info in titles.items():
                    if entity in title:
                        url = info.get("url", "")
                        mobile_url = info.get("mobileUrl", "")
                        ranks = info.get("ranks", [])
                        count = len(ranks)

                        related_news.append({
                            "title": title,
                            "platform": platform_id,
                            "platform_name": platform_name,
                            "url": url,
                            "mobileUrl": mobile_url,
                            "ranks": ranks,
                            "count": count,
                            "rank": ranks[0] if ranks else 999
                        })

                        # æå–å®ä½“å‘¨è¾¹çš„å…³é”®è¯
                        keywords = self._extract_keywords(title)
                        entity_context.update(keywords)

            if not related_news:
                raise DataNotFoundError(
                    f"æœªæ‰¾åˆ°åŒ…å«å®ä½“ '{entity}' çš„æ–°é—»",
                    suggestion="è¯·å°è¯•å…¶ä»–å®ä½“åç§°"
                )

            # ç§»é™¤å®ä½“æœ¬èº«
            if entity in entity_context:
                del entity_context[entity]

            # æŒ‰æƒé‡æ’åºï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if sort_by_weight:
                related_news.sort(
                    key=lambda x: calculate_news_weight(x),
                    reverse=True
                )
            else:
                # æŒ‰æ’åæ’åº
                related_news.sort(key=lambda x: x["rank"])

            # é™åˆ¶è¿”å›æ•°é‡
            result_news = related_news[:limit]

            return {
                "success": True,
                "summary": {
                    "description": f"å®ä½“ã€Œ{entity}ã€ç›¸å…³æ–°é—»",
                    "entity": entity,
                    "entity_type": entity_type or "auto",
                    "total_found": len(related_news),
                    "returned": len(result_news),
                    "sorted_by_weight": sort_by_weight
                },
                "data": result_news,
                "related_keywords": [
                    {"keyword": k, "count": v}
                    for k, v in entity_context.most_common(10)
                ]
            }

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def generate_summary_report(
        self,
        report_type: str = "daily",
        date_range: Optional[Union[Dict[str, str], str]] = None
    ) -> Dict:
        """
        æ¯æ—¥/æ¯å‘¨æ‘˜è¦ç”Ÿæˆå™¨ - è‡ªåŠ¨ç”Ÿæˆçƒ­ç‚¹æ‘˜è¦æŠ¥å‘Š

        Args:
            report_type: æŠ¥å‘Šç±»å‹ï¼ˆdaily/weeklyï¼‰
            date_range: è‡ªå®šä¹‰æ—¥æœŸèŒƒå›´ï¼ˆå¯é€‰ï¼‰

        Returns:
            Markdownæ ¼å¼çš„æ‘˜è¦æŠ¥å‘Š

        Examples:
            ç”¨æˆ·è¯¢é—®ç¤ºä¾‹ï¼š
            - "ç”Ÿæˆä»Šå¤©çš„æ–°é—»æ‘˜è¦æŠ¥å‘Š"
            - "ç»™æˆ‘ä¸€ä»½æœ¬å‘¨çš„çƒ­ç‚¹æ€»ç»“"
            - "ç”Ÿæˆè¿‡å»7å¤©çš„æ–°é—»åˆ†ææŠ¥å‘Š"

            ä»£ç è°ƒç”¨ç¤ºä¾‹ï¼š
            >>> tools = AnalyticsTools()
            >>> result = tools.generate_summary_report(
            ...     report_type="daily"
            ... )
            >>> print(result['markdown_report'])
        """
        try:
            # å‚æ•°éªŒè¯
            if report_type not in ["daily", "weekly"]:
                raise InvalidParameterError(
                    f"æ— æ•ˆçš„æŠ¥å‘Šç±»å‹: {report_type}",
                    suggestion="æ”¯æŒçš„ç±»å‹: daily, weekly"
                )

            # ç¡®å®šæ—¥æœŸèŒƒå›´
            if date_range:
                date_range_tuple = validate_date_range(date_range)
                start_date, end_date = date_range_tuple
            else:
                if report_type == "daily":
                    start_date = end_date = datetime.now()
                else:  # weekly
                    end_date = datetime.now()
                    start_date = end_date - timedelta(days=6)

            # æ”¶é›†æ•°æ®
            all_keywords = Counter()
            all_platforms_news = defaultdict(int)
            all_titles_list = []

            current_date = start_date
            while current_date <= end_date:
                try:
                    all_titles, id_to_name, _ = self.data_service.parser.read_all_titles_for_date(
                        date=current_date
                    )

                    for platform_id, titles in all_titles.items():
                        platform_name = id_to_name.get(platform_id, platform_id)
                        all_platforms_news[platform_name] += len(titles)

                        for title in titles.keys():
                            all_titles_list.append({
                                "title": title,
                                "platform": platform_name,
                                "date": current_date.strftime("%Y-%m-%d")
                            })

                            # æå–å…³é”®è¯
                            keywords = self._extract_keywords(title)
                            all_keywords.update(keywords)

                except DataNotFoundError:
                    pass

                current_date += timedelta(days=1)

            # ç”ŸæˆæŠ¥å‘Š
            report_title = f"{'æ¯æ—¥' if report_type == 'daily' else 'æ¯å‘¨'}æ–°é—»çƒ­ç‚¹æ‘˜è¦"
            date_str = f"{start_date.strftime('%Y-%m-%d')}" if report_type == "daily" else f"{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}"

            # æ„å»ºMarkdownæŠ¥å‘Š
            markdown = f"""# {report_title}

**æŠ¥å‘Šæ—¥æœŸ**: {date_str}
**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ğŸ“Š æ•°æ®æ¦‚è§ˆ

- **æ€»æ–°é—»æ•°**: {len(all_titles_list)}
- **è¦†ç›–å¹³å°**: {len(all_platforms_news)}
- **çƒ­é—¨å…³é”®è¯æ•°**: {len(all_keywords)}

## ğŸ”¥ TOP 10 çƒ­é—¨è¯é¢˜

"""

            # æ·»åŠ TOP 10å…³é”®è¯
            for i, (keyword, count) in enumerate(all_keywords.most_common(10), 1):
                markdown += f"{i}. **{keyword}** - å‡ºç° {count} æ¬¡\n"

            # å¹³å°åˆ†æ
            markdown += "\n## ğŸ“± å¹³å°æ´»è·ƒåº¦\n\n"
            sorted_platforms = sorted(all_platforms_news.items(), key=lambda x: x[1], reverse=True)

            for platform, count in sorted_platforms:
                markdown += f"- **{platform}**: {count} æ¡æ–°é—»\n"

            # è¶‹åŠ¿å˜åŒ–ï¼ˆå¦‚æœæ˜¯å‘¨æŠ¥ï¼‰
            if report_type == "weekly":
                markdown += "\n## ğŸ“ˆ è¶‹åŠ¿åˆ†æ\n\n"
                markdown += "æœ¬å‘¨çƒ­åº¦æŒç»­çš„è¯é¢˜ï¼ˆæ ·æœ¬æ•°æ®ï¼‰ï¼š\n\n"

                # ç®€å•çš„è¶‹åŠ¿åˆ†æ
                top_keywords = [kw for kw, _ in all_keywords.most_common(5)]
                for keyword in top_keywords:
                    markdown += f"- **{keyword}**: æŒç»­çƒ­é—¨\n"

            # æ·»åŠ æ ·æœ¬æ–°é—»ï¼ˆæŒ‰æƒé‡é€‰æ‹©ï¼Œç¡®ä¿ç¡®å®šæ€§ï¼‰
            markdown += "\n## ğŸ“° ç²¾é€‰æ–°é—»æ ·æœ¬\n\n"

            # ç¡®å®šæ€§é€‰å–ï¼šæŒ‰æ ‡é¢˜çš„æƒé‡æ’åºï¼Œå–å‰5æ¡
            # è¿™æ ·ç›¸åŒè¾“å…¥æ€»æ˜¯è¿”å›ç›¸åŒç»“æœ
            if all_titles_list:
                # è®¡ç®—æ¯æ¡æ–°é—»çš„æƒé‡åˆ†æ•°ï¼ˆåŸºäºå…³é”®è¯å‡ºç°æ¬¡æ•°ï¼‰
                news_with_scores = []
                for news in all_titles_list:
                    # ç®€å•æƒé‡ï¼šç»Ÿè®¡åŒ…å«TOPå…³é”®è¯çš„æ¬¡æ•°
                    score = 0
                    title_lower = news['title'].lower()
                    for keyword, count in all_keywords.most_common(10):
                        if keyword.lower() in title_lower:
                            score += count
                    news_with_scores.append((news, score))

                # æŒ‰æƒé‡é™åºæ’åºï¼Œæƒé‡ç›¸åŒåˆ™æŒ‰æ ‡é¢˜å­—æ¯é¡ºåºï¼ˆç¡®ä¿ç¡®å®šæ€§ï¼‰
                news_with_scores.sort(key=lambda x: (-x[1], x[0]['title']))

                # å–å‰5æ¡
                sample_news = [item[0] for item in news_with_scores[:5]]

                for news in sample_news:
                    markdown += f"- [{news['platform']}] {news['title']}\n"

            markdown += "\n---\n\n*æœ¬æŠ¥å‘Šç”± TrendRadar MCP è‡ªåŠ¨ç”Ÿæˆ*\n"

            return {
                "success": True,
                "report_type": report_type,
                "date_range": {
                    "start": start_date.strftime("%Y-%m-%d"),
                    "end": end_date.strftime("%Y-%m-%d")
                },
                "markdown_report": markdown,
                "statistics": {
                    "total_news": len(all_titles_list),
                    "platforms_count": len(all_platforms_news),
                    "keywords_count": len(all_keywords),
                    "top_keyword": all_keywords.most_common(1)[0] if all_keywords else None
                }
            }

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def get_platform_activity_stats(
        self,
        date_range: Optional[Union[Dict[str, str], str]] = None
    ) -> Dict:
        """
        å¹³å°æ´»è·ƒåº¦ç»Ÿè®¡ - ç»Ÿè®¡å„å¹³å°çš„å‘å¸ƒé¢‘ç‡å’Œæ´»è·ƒæ—¶é—´æ®µ

        Args:
            date_range: æ—¥æœŸèŒƒå›´ï¼ˆå¯é€‰ï¼‰

        Returns:
            å¹³å°æ´»è·ƒåº¦ç»Ÿè®¡ç»“æœ

        Examples:
            ç”¨æˆ·è¯¢é—®ç¤ºä¾‹ï¼š
            - "ç»Ÿè®¡å„å¹³å°ä»Šå¤©çš„æ´»è·ƒåº¦"
            - "çœ‹çœ‹å“ªä¸ªå¹³å°æ›´æ–°æœ€é¢‘ç¹"
            - "åˆ†æå„å¹³å°çš„å‘å¸ƒæ—¶é—´è§„å¾‹"

            ä»£ç è°ƒç”¨ç¤ºä¾‹ï¼š
            >>> # æŸ¥çœ‹å„å¹³å°æ´»è·ƒåº¦ï¼ˆå‡è®¾ä»Šå¤©æ˜¯ 2025-11-17ï¼‰
            >>> result = tools.get_platform_activity_stats(
            ...     date_range={"start": "2025-11-08", "end": "2025-11-17"}
            ... )
            >>> print(result['platform_activity'])
        """
        try:
            # å‚æ•°éªŒè¯
            date_range_tuple = validate_date_range(date_range)

            # ç¡®å®šæ—¥æœŸèŒƒå›´
            if date_range_tuple:
                start_date, end_date = date_range_tuple
            else:
                start_date = end_date = datetime.now()

            # ç»Ÿè®¡å„å¹³å°æ´»è·ƒåº¦
            platform_activity = defaultdict(lambda: {
                "total_updates": 0,
                "days_active": set(),
                "news_count": 0,
                "hourly_distribution": Counter()
            })

            # éå†æ—¥æœŸèŒƒå›´
            current_date = start_date
            while current_date <= end_date:
                try:
                    all_titles, id_to_name, timestamps = self.data_service.parser.read_all_titles_for_date(
                        date=current_date
                    )

                    for platform_id, titles in all_titles.items():
                        platform_name = id_to_name.get(platform_id, platform_id)

                        platform_activity[platform_name]["news_count"] += len(titles)
                        platform_activity[platform_name]["days_active"].add(current_date.strftime("%Y-%m-%d"))

                        # ç»Ÿè®¡æ›´æ–°æ¬¡æ•°ï¼ˆåŸºäºæ–‡ä»¶æ•°é‡ï¼‰
                        platform_activity[platform_name]["total_updates"] += len(timestamps)

                        # ç»Ÿè®¡æ—¶é—´åˆ†å¸ƒï¼ˆåŸºäºæ–‡ä»¶åä¸­çš„æ—¶é—´ï¼‰
                        for filename in timestamps.keys():
                            # è§£ææ–‡ä»¶åä¸­çš„å°æ—¶ï¼ˆæ ¼å¼ï¼šHHMM.txtï¼‰
                            match = re.match(r'(\d{2})(\d{2})\.txt', filename)
                            if match:
                                hour = int(match.group(1))
                                platform_activity[platform_name]["hourly_distribution"][hour] += 1

                except DataNotFoundError:
                    pass

                current_date += timedelta(days=1)

            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            result_activity = {}
            for platform, stats in platform_activity.items():
                days_count = len(stats["days_active"])
                avg_news_per_day = stats["news_count"] / days_count if days_count > 0 else 0

                # æ‰¾å‡ºæœ€æ´»è·ƒçš„æ—¶é—´æ®µ
                most_active_hours = stats["hourly_distribution"].most_common(3)

                result_activity[platform] = {
                    "total_updates": stats["total_updates"],
                    "news_count": stats["news_count"],
                    "days_active": days_count,
                    "avg_news_per_day": round(avg_news_per_day, 2),
                    "most_active_hours": [
                        {"hour": f"{hour:02d}:00", "count": count}
                        for hour, count in most_active_hours
                    ],
                    "activity_score": round(stats["news_count"] / max(days_count, 1), 2)
                }

            # æŒ‰æ´»è·ƒåº¦æ’åº
            sorted_platforms = sorted(
                result_activity.items(),
                key=lambda x: x[1]["activity_score"],
                reverse=True
            )

            return {
                "success": True,
                "date_range": {
                    "start": start_date.strftime("%Y-%m-%d"),
                    "end": end_date.strftime("%Y-%m-%d")
                },
                "platform_activity": dict(sorted_platforms),
                "most_active_platform": sorted_platforms[0][0] if sorted_platforms else None,
                "total_platforms": len(result_activity)
            }

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def analyze_topic_lifecycle(
        self,
        topic: str,
        date_range: Optional[Union[Dict[str, str], str]] = None
    ) -> Dict:
        """
        è¯é¢˜ç”Ÿå‘½å‘¨æœŸåˆ†æ - è¿½è¸ªè¯é¢˜ä»å‡ºç°åˆ°æ¶ˆå¤±çš„å®Œæ•´å‘¨æœŸ

        Args:
            topic: è¯é¢˜å…³é”®è¯
            date_range: æ—¥æœŸèŒƒå›´ï¼ˆå¯é€‰ï¼‰
                       - **æ ¼å¼**: {"start": "YYYY-MM-DD", "end": "YYYY-MM-DD"}
                       - **é»˜è®¤**: ä¸æŒ‡å®šæ—¶é»˜è®¤åˆ†ææœ€è¿‘7å¤©

        Returns:
            è¯é¢˜ç”Ÿå‘½å‘¨æœŸåˆ†æç»“æœ

        Examples:
            ç”¨æˆ·è¯¢é—®ç¤ºä¾‹ï¼š
            - "åˆ†æ'äººå·¥æ™ºèƒ½'è¿™ä¸ªè¯é¢˜çš„ç”Ÿå‘½å‘¨æœŸ"
            - "çœ‹çœ‹'iPhone'è¯é¢˜æ˜¯æ˜™èŠ±ä¸€ç°è¿˜æ˜¯æŒç»­çƒ­ç‚¹"
            - "è¿½è¸ª'æ¯”ç‰¹å¸'è¯é¢˜çš„çƒ­åº¦å˜åŒ–"

            ä»£ç è°ƒç”¨ç¤ºä¾‹ï¼š
            >>> # åˆ†æè¯é¢˜ç”Ÿå‘½å‘¨æœŸï¼ˆå‡è®¾ä»Šå¤©æ˜¯ 2025-11-17ï¼‰
            >>> result = tools.analyze_topic_lifecycle(
            ...     topic="äººå·¥æ™ºèƒ½",
            ...     date_range={"start": "2025-10-19", "end": "2025-11-17"}
            ... )
            >>> print(result['lifecycle_stage'])
        """
        try:
            # å‚æ•°éªŒè¯
            topic = validate_keyword(topic)

            # å¤„ç†æ—¥æœŸèŒƒå›´ï¼ˆä¸æŒ‡å®šæ—¶é»˜è®¤æœ€è¿‘7å¤©ï¼‰
            if date_range:
                from ..utils.validators import validate_date_range
                date_range_tuple = validate_date_range(date_range)
                start_date, end_date = date_range_tuple
            else:
                # é»˜è®¤æœ€è¿‘7å¤©
                end_date = datetime.now()
                start_date = end_date - timedelta(days=6)

            # æ”¶é›†è¯é¢˜å†å²æ•°æ®
            lifecycle_data = []
            current_date = start_date
            while current_date <= end_date:
                try:
                    all_titles, _, _ = self.data_service.parser.read_all_titles_for_date(
                        date=current_date
                    )

                    # ç»Ÿè®¡è¯¥æ—¥çš„è¯é¢˜å‡ºç°æ¬¡æ•°
                    count = 0
                    for _, titles in all_titles.items():
                        for title in titles.keys():
                            if topic.lower() in title.lower():
                                count += 1

                    lifecycle_data.append({
                        "date": current_date.strftime("%Y-%m-%d"),
                        "count": count
                    })

                except DataNotFoundError:
                    lifecycle_data.append({
                        "date": current_date.strftime("%Y-%m-%d"),
                        "count": 0
                    })

                current_date += timedelta(days=1)

            # è®¡ç®—åˆ†æå¤©æ•°
            total_days = (end_date - start_date).days + 1

            # åˆ†æç”Ÿå‘½å‘¨æœŸé˜¶æ®µ
            counts = [item["count"] for item in lifecycle_data]

            if not any(counts):
                time_desc = f"{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}"
                raise DataNotFoundError(
                    f"åœ¨ {time_desc} å†…æœªæ‰¾åˆ°è¯é¢˜ '{topic}'",
                    suggestion="è¯·å°è¯•å…¶ä»–è¯é¢˜æˆ–æ‰©å¤§æ—¶é—´èŒƒå›´"
                )

            # æ‰¾åˆ°é¦–æ¬¡å‡ºç°å’Œæœ€åå‡ºç°
            first_appearance = next((item["date"] for item in lifecycle_data if item["count"] > 0), None)
            last_appearance = next((item["date"] for item in reversed(lifecycle_data) if item["count"] > 0), None)

            # è®¡ç®—å³°å€¼
            max_count = max(counts)
            peak_index = counts.index(max_count)
            peak_date = lifecycle_data[peak_index]["date"]

            # è®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆç®€å•å®ç°ï¼‰
            non_zero_counts = [c for c in counts if c > 0]
            avg_count = sum(non_zero_counts) / len(non_zero_counts) if non_zero_counts else 0

            # åˆ¤æ–­ç”Ÿå‘½å‘¨æœŸé˜¶æ®µ
            recent_counts = counts[-3:]  # æœ€è¿‘3å¤©
            early_counts = counts[:3]    # å‰3å¤©

            if sum(recent_counts) > sum(early_counts):
                lifecycle_stage = "ä¸Šå‡æœŸ"
            elif sum(recent_counts) < sum(early_counts) * 0.5:
                lifecycle_stage = "è¡°é€€æœŸ"
            elif max_count in recent_counts:
                lifecycle_stage = "çˆ†å‘æœŸ"
            else:
                lifecycle_stage = "ç¨³å®šæœŸ"

            # åˆ†ç±»ï¼šæ˜™èŠ±ä¸€ç° vs æŒç»­çƒ­ç‚¹
            active_days = sum(1 for c in counts if c > 0)

            if active_days <= 2 and max_count > avg_count * 2:
                topic_type = "æ˜™èŠ±ä¸€ç°"
            elif active_days >= total_days * 0.6:
                topic_type = "æŒç»­çƒ­ç‚¹"
            else:
                topic_type = "å‘¨æœŸæ€§çƒ­ç‚¹"

            return {
                "success": True,
                "topic": topic,
                "date_range": {
                    "start": start_date.strftime("%Y-%m-%d"),
                    "end": end_date.strftime("%Y-%m-%d"),
                    "total_days": total_days
                },
                "lifecycle_data": lifecycle_data,
                "analysis": {
                    "first_appearance": first_appearance,
                    "last_appearance": last_appearance,
                    "peak_date": peak_date,
                    "peak_count": max_count,
                    "active_days": active_days,
                    "avg_daily_mentions": round(avg_count, 2),
                    "lifecycle_stage": lifecycle_stage,
                    "topic_type": topic_type
                }
            }

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def detect_viral_topics(
        self,
        threshold: float = 3.0,
        time_window: int = 24
    ) -> Dict:
        """
        å¼‚å¸¸çƒ­åº¦æ£€æµ‹ - è‡ªåŠ¨è¯†åˆ«çªç„¶çˆ†ç«çš„è¯é¢˜

        Args:
            threshold: çƒ­åº¦çªå¢å€æ•°é˜ˆå€¼
            time_window: æ£€æµ‹æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰

        Returns:
            çˆ†ç«è¯é¢˜åˆ—è¡¨

        Examples:
            ç”¨æˆ·è¯¢é—®ç¤ºä¾‹ï¼š
            - "æ£€æµ‹ä»Šå¤©æœ‰å“ªäº›çªç„¶çˆ†ç«çš„è¯é¢˜"
            - "çœ‹çœ‹æœ‰æ²¡æœ‰çƒ­åº¦å¼‚å¸¸çš„æ–°é—»"
            - "é¢„è­¦å¯èƒ½çš„é‡å¤§äº‹ä»¶"

            ä»£ç è°ƒç”¨ç¤ºä¾‹ï¼š
            >>> tools = AnalyticsTools()
            >>> result = tools.detect_viral_topics(
            ...     threshold=3.0,
            ...     time_window=24
            ... )
            >>> print(result['viral_topics'])
        """
        try:
            # å‚æ•°éªŒè¯
            threshold = validate_threshold(threshold, default=3.0, min_value=1.0, max_value=100.0)
            time_window = validate_limit(time_window, default=24, max_limit=72)

            # è¯»å–å½“å‰å’Œä¹‹å‰çš„æ•°æ®
            current_all_titles, _, _ = self.data_service.parser.read_all_titles_for_date()

            # è¯»å–æ˜¨å¤©çš„æ•°æ®ä½œä¸ºåŸºå‡†
            yesterday = datetime.now() - timedelta(days=1)
            try:
                previous_all_titles, _, _ = self.data_service.parser.read_all_titles_for_date(
                    date=yesterday
                )
            except DataNotFoundError:
                previous_all_titles = {}

            # ç»Ÿè®¡å½“å‰çš„å…³é”®è¯é¢‘ç‡
            current_keywords = Counter()
            current_keyword_titles = defaultdict(list)

            for _, titles in current_all_titles.items():
                for title in titles.keys():
                    keywords = self._extract_keywords(title)
                    current_keywords.update(keywords)

                    for kw in keywords:
                        current_keyword_titles[kw].append(title)

            # ç»Ÿè®¡ä¹‹å‰çš„å…³é”®è¯é¢‘ç‡
            previous_keywords = Counter()

            for _, titles in previous_all_titles.items():
                for title in titles.keys():
                    keywords = self._extract_keywords(title)
                    previous_keywords.update(keywords)

            # æ£€æµ‹å¼‚å¸¸çƒ­åº¦
            viral_topics = []

            for keyword, current_count in current_keywords.items():
                previous_count = previous_keywords.get(keyword, 0)

                # è®¡ç®—å¢é•¿å€æ•°
                if previous_count == 0:
                    # æ–°å‡ºç°çš„è¯é¢˜
                    if current_count >= 5:  # è‡³å°‘å‡ºç°5æ¬¡æ‰è®¤ä¸ºæ˜¯çˆ†ç«
                        growth_rate = float('inf')
                        is_viral = True
                    else:
                        continue
                else:
                    growth_rate = current_count / previous_count
                    is_viral = growth_rate >= threshold

                if is_viral:
                    viral_topics.append({
                        "keyword": keyword,
                        "current_count": current_count,
                        "previous_count": previous_count,
                        "growth_rate": round(growth_rate, 2) if growth_rate != float('inf') else "æ–°è¯é¢˜",
                        "sample_titles": current_keyword_titles[keyword][:3],
                        "alert_level": "é«˜" if growth_rate > threshold * 2 else "ä¸­"
                    })

            # æŒ‰å¢é•¿ç‡æ’åº
            viral_topics.sort(
                key=lambda x: x["current_count"] if x["growth_rate"] == "æ–°è¯é¢˜" else x["growth_rate"],
                reverse=True
            )

            if not viral_topics:
                return {
                    "success": True,
                    "summary": {
                        "description": "å¼‚å¸¸çƒ­åº¦æ£€æµ‹ç»“æœ",
                        "total": 0,
                        "threshold": threshold,
                        "time_window": time_window
                    },
                    "data": [],
                    "message": f"æœªæ£€æµ‹åˆ°çƒ­åº¦å¢é•¿è¶…è¿‡ {threshold} å€çš„è¯é¢˜"
                }

            return {
                "success": True,
                "summary": {
                    "description": "å¼‚å¸¸çƒ­åº¦æ£€æµ‹ç»“æœ",
                    "total": len(viral_topics),
                    "threshold": threshold,
                    "time_window": time_window,
                    "detection_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                },
                "data": viral_topics
            }

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def predict_trending_topics(
        self,
        lookahead_hours: int = 6,
        confidence_threshold: float = 0.7
    ) -> Dict:
        """
        è¯é¢˜é¢„æµ‹ - åŸºäºå†å²æ•°æ®é¢„æµ‹æœªæ¥å¯èƒ½çš„çƒ­ç‚¹

        Args:
            lookahead_hours: é¢„æµ‹æœªæ¥å¤šå°‘å°æ—¶
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼

        Returns:
            é¢„æµ‹çš„æ½œåŠ›è¯é¢˜åˆ—è¡¨

        Examples:
            ç”¨æˆ·è¯¢é—®ç¤ºä¾‹ï¼š
            - "é¢„æµ‹æ¥ä¸‹æ¥6å°æ—¶å¯èƒ½çš„çƒ­ç‚¹è¯é¢˜"
            - "æœ‰å“ªäº›è¯é¢˜å¯èƒ½ä¼šç«èµ·æ¥"
            - "æ—©æœŸå‘ç°æ½œåŠ›è¯é¢˜"

            ä»£ç è°ƒç”¨ç¤ºä¾‹ï¼š
            >>> tools = AnalyticsTools()
            >>> result = tools.predict_trending_topics(
            ...     lookahead_hours=6,
            ...     confidence_threshold=0.7
            ... )
            >>> print(result['predicted_topics'])
        """
        try:
            # å‚æ•°éªŒè¯
            lookahead_hours = validate_limit(lookahead_hours, default=6, max_limit=48)
            confidence_threshold = validate_threshold(
                confidence_threshold,
                default=0.7,
                min_value=0.0,
                max_value=1.0,
                param_name="confidence_threshold"
            )

            # æ”¶é›†æœ€è¿‘3å¤©çš„æ•°æ®ç”¨äºé¢„æµ‹
            keyword_trends = defaultdict(list)

            for days_ago in range(3, 0, -1):
                date = datetime.now() - timedelta(days=days_ago)

                try:
                    all_titles, _, _ = self.data_service.parser.read_all_titles_for_date(
                        date=date
                    )

                    # ç»Ÿè®¡å…³é”®è¯
                    keywords_count = Counter()
                    for _, titles in all_titles.items():
                        for title in titles.keys():
                            keywords = self._extract_keywords(title)
                            keywords_count.update(keywords)

                    # è®°å½•æ¯ä¸ªå…³é”®è¯çš„å†å²æ•°æ®
                    for keyword, count in keywords_count.items():
                        keyword_trends[keyword].append(count)

                except DataNotFoundError:
                    pass

            # æ·»åŠ ä»Šå¤©çš„æ•°æ®
            try:
                all_titles, _, _ = self.data_service.parser.read_all_titles_for_date()

                keywords_count = Counter()
                keyword_titles = defaultdict(list)

                for _, titles in all_titles.items():
                    for title in titles.keys():
                        keywords = self._extract_keywords(title)
                        keywords_count.update(keywords)

                        for kw in keywords:
                            keyword_titles[kw].append(title)

                for keyword, count in keywords_count.items():
                    keyword_trends[keyword].append(count)

            except DataNotFoundError:
                raise DataNotFoundError(
                    "æœªæ‰¾åˆ°ä»Šå¤©çš„æ•°æ®",
                    suggestion="è¯·ç­‰å¾…çˆ¬è™«ä»»åŠ¡å®Œæˆ"
                )

            # é¢„æµ‹æ½œåŠ›è¯é¢˜
            predicted_topics = []

            for keyword, trend_data in keyword_trends.items():
                if len(trend_data) < 2:
                    continue

                # ç®€å•çš„çº¿æ€§è¶‹åŠ¿é¢„æµ‹
                # è®¡ç®—å¢é•¿ç‡
                recent_value = trend_data[-1]
                previous_value = trend_data[-2] if len(trend_data) >= 2 else 0

                if previous_value == 0:
                    if recent_value >= 3:
                        growth_rate = 1.0
                    else:
                        continue
                else:
                    growth_rate = (recent_value - previous_value) / previous_value

                # åˆ¤æ–­æ˜¯å¦æ˜¯ä¸Šå‡è¶‹åŠ¿
                if growth_rate > 0.3:  # å¢é•¿è¶…è¿‡30%
                    # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºè¶‹åŠ¿çš„ç¨³å®šæ€§ï¼‰
                    if len(trend_data) >= 3:
                        # æ£€æŸ¥æ˜¯å¦è¿ç»­å¢é•¿
                        is_consistent = all(
                            trend_data[i] <= trend_data[i+1]
                            for i in range(len(trend_data)-1)
                        )
                        confidence = 0.9 if is_consistent else 0.7
                    else:
                        confidence = 0.6

                    if confidence >= confidence_threshold:
                        predicted_topics.append({
                            "keyword": keyword,
                            "current_count": recent_value,
                            "growth_rate": round(growth_rate * 100, 2),
                            "confidence": round(confidence, 2),
                            "trend_data": trend_data,
                            "prediction": "ä¸Šå‡è¶‹åŠ¿ï¼Œå¯èƒ½æˆä¸ºçƒ­ç‚¹",
                            "sample_titles": keyword_titles.get(keyword, [])[:3]
                        })

            # æŒ‰ç½®ä¿¡åº¦å’Œå¢é•¿ç‡æ’åº
            predicted_topics.sort(
                key=lambda x: (x["confidence"], x["growth_rate"]),
                reverse=True
            )

            return {
                "success": True,
                "summary": {
                    "description": "çƒ­ç‚¹è¯é¢˜é¢„æµ‹ç»“æœ",
                    "total": len(predicted_topics),
                    "returned": min(20, len(predicted_topics)),
                    "lookahead_hours": lookahead_hours,
                    "confidence_threshold": confidence_threshold,
                    "prediction_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                },
                "data": predicted_topics[:20],  # è¿”å›TOP 20
                "note": "é¢„æµ‹åŸºäºå†å²è¶‹åŠ¿ï¼Œå®é™…ç»“æœå¯èƒ½æœ‰åå·®"
            }

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    # ==================== è¾…åŠ©æ–¹æ³• ====================

    def _extract_keywords(self, title: str, min_length: int = 2) -> List[str]:
        """
        ä»æ ‡é¢˜ä¸­æå–å…³é”®è¯ï¼ˆç®€å•å®ç°ï¼‰

        Args:
            title: æ ‡é¢˜æ–‡æœ¬
            min_length: æœ€å°å…³é”®è¯é•¿åº¦

        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        # ç§»é™¤URLå’Œç‰¹æ®Šå­—ç¬¦
        title = re.sub(r'http[s]?://\S+', '', title)
        title = re.sub(r'[^\w\s]', ' ', title)

        # ç®€å•åˆ†è¯ï¼ˆæŒ‰ç©ºæ ¼å’Œå¸¸è§åˆ†éš”ç¬¦ï¼‰
        words = re.split(r'[\sï¼Œã€‚ï¼ï¼Ÿã€]+', title)

        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        stopwords = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}

        keywords = [
            word.strip() for word in words
            if word.strip() and len(word.strip()) >= min_length and word.strip() not in stopwords
        ]

        return keywords

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦

        Args:
            text1: æ–‡æœ¬1
            text2: æ–‡æœ¬2

        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ä¹‹é—´ï¼‰
        """
        # ä½¿ç”¨ SequenceMatcher è®¡ç®—ç›¸ä¼¼åº¦
        return SequenceMatcher(None, text1, text2).ratio()

    def _find_unique_topics(self, platform_stats: Dict) -> Dict[str, List[str]]:
        """
        æ‰¾å‡ºå„å¹³å°ç‹¬æœ‰çš„çƒ­ç‚¹è¯é¢˜

        Args:
            platform_stats: å¹³å°ç»Ÿè®¡æ•°æ®

        Returns:
            å„å¹³å°ç‹¬æœ‰è¯é¢˜å­—å…¸
        """
        unique_topics = {}

        # è·å–æ¯ä¸ªå¹³å°çš„TOPå…³é”®è¯
        platform_keywords = {}
        for platform, stats in platform_stats.items():
            top_keywords = set([kw for kw, _ in stats["top_keywords"].most_common(10)])
            platform_keywords[platform] = top_keywords

        # æ‰¾å‡ºç‹¬æœ‰å…³é”®è¯
        for platform, keywords in platform_keywords.items():
            # æ‰¾å‡ºå…¶ä»–å¹³å°çš„æ‰€æœ‰å…³é”®è¯
            other_keywords = set()
            for other_platform, other_kws in platform_keywords.items():
                if other_platform != platform:
                    other_keywords.update(other_kws)

            # æ‰¾å‡ºç‹¬æœ‰çš„
            unique = keywords - other_keywords
            if unique:
                unique_topics[platform] = list(unique)[:5]  # æœ€å¤š5ä¸ª

        return unique_topics

    # ==================== è·¨å¹³å°èšåˆå·¥å…· ====================

    def aggregate_news(
        self,
        date_range: Optional[Union[Dict[str, str], str]] = None,
        platforms: Optional[List[str]] = None,
        similarity_threshold: float = 0.7,
        limit: int = 50,
        include_url: bool = False
    ) -> Dict:
        """
        è·¨å¹³å°æ–°é—»èšåˆ - å¯¹ç›¸ä¼¼æ–°é—»è¿›è¡Œå»é‡åˆå¹¶

        å°†ä¸åŒå¹³å°æŠ¥é“çš„åŒä¸€äº‹ä»¶åˆå¹¶ä¸ºä¸€æ¡èšåˆæ–°é—»ï¼Œ
        æ˜¾ç¤ºè¯¥æ–°é—»åœ¨å„å¹³å°çš„è¦†ç›–æƒ…å†µå’Œç»¼åˆçƒ­åº¦ã€‚

        Args:
            date_range: æ—¥æœŸèŒƒå›´ï¼ˆå¯é€‰ï¼‰
                - ä¸æŒ‡å®š: æŸ¥è¯¢ä»Šå¤©
                - {\"start\": \"YYYY-MM-DD\", \"end\": \"YYYY-MM-DD\"}: æ—¥æœŸèŒƒå›´
            platforms: å¹³å°è¿‡æ»¤åˆ—è¡¨ï¼Œå¦‚ ['zhihu', 'weibo']
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œ0-1ä¹‹é—´ï¼Œé»˜è®¤0.7
            limit: è¿”å›èšåˆæ–°é—»æ•°é‡ï¼Œé»˜è®¤50
            include_url: æ˜¯å¦åŒ…å«URLé“¾æ¥ï¼Œé»˜è®¤False

        Returns:
            èšåˆç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
            - aggregated_news: èšåˆåçš„æ–°é—»åˆ—è¡¨
            - statistics: èšåˆç»Ÿè®¡ä¿¡æ¯
        """
        try:
            # å‚æ•°éªŒè¯
            platforms = validate_platforms(platforms)
            similarity_threshold = validate_threshold(
                similarity_threshold, default=0.7, min_value=0.3, max_value=1.0
            )
            limit = validate_limit(limit, default=50)

            # å¤„ç†æ—¥æœŸèŒƒå›´
            if date_range:
                date_range_tuple = validate_date_range(date_range)
                start_date, end_date = date_range_tuple
            else:
                start_date = end_date = datetime.now()

            # æ”¶é›†æ‰€æœ‰æ–°é—»
            all_news = []
            current_date = start_date

            while current_date <= end_date:
                try:
                    all_titles, id_to_name, _ = self.data_service.parser.read_all_titles_for_date(
                        date=current_date,
                        platform_ids=platforms
                    )

                    for platform_id, titles in all_titles.items():
                        platform_name = id_to_name.get(platform_id, platform_id)

                        for title, info in titles.items():
                            news_item = {
                                "title": title,
                                "platform": platform_id,
                                "platform_name": platform_name,
                                "date": current_date.strftime("%Y-%m-%d"),
                                "ranks": info.get("ranks", []),
                                "count": len(info.get("ranks", [])),
                                "rank": info["ranks"][0] if info["ranks"] else 999
                            }

                            if include_url:
                                news_item["url"] = info.get("url", "")
                                news_item["mobileUrl"] = info.get("mobileUrl", "")

                            # è®¡ç®—æƒé‡
                            news_item["weight"] = calculate_news_weight(news_item)
                            all_news.append(news_item)

                except DataNotFoundError:
                    pass

                current_date += timedelta(days=1)

            if not all_news:
                return {
                    "success": True,
                    "summary": {
                        "description": "è·¨å¹³å°æ–°é—»èšåˆç»“æœ",
                        "total": 0,
                        "returned": 0
                    },
                    "data": [],
                    "message": "æœªæ‰¾åˆ°æ–°é—»æ•°æ®"
                }

            # æ‰§è¡Œèšåˆ
            aggregated = self._aggregate_similar_news(
                all_news, similarity_threshold, include_url
            )

            # æŒ‰ç»¼åˆæƒé‡æ’åº
            aggregated.sort(key=lambda x: x["aggregate_weight"], reverse=True)

            # é™åˆ¶è¿”å›æ•°é‡
            results = aggregated[:limit]

            # ç»Ÿè®¡ä¿¡æ¯
            total_original = len(all_news)
            total_aggregated = len(aggregated)
            dedup_rate = 1 - (total_aggregated / total_original) if total_original > 0 else 0

            platform_coverage = Counter()
            for item in aggregated:
                for p in item["platforms"]:
                    platform_coverage[p] += 1

            return {
                "success": True,
                "summary": {
                    "description": "è·¨å¹³å°æ–°é—»èšåˆç»“æœ",
                    "original_count": total_original,
                    "aggregated_count": total_aggregated,
                    "returned": len(results),
                    "deduplication_rate": f"{dedup_rate * 100:.1f}%",
                    "similarity_threshold": similarity_threshold,
                    "date_range": {
                        "start": start_date.strftime("%Y-%m-%d"),
                        "end": end_date.strftime("%Y-%m-%d")
                    }
                },
                "data": results,
                "statistics": {
                    "platform_coverage": dict(platform_coverage),
                    "multi_platform_news": len([a for a in aggregated if len(a["platforms"]) > 1]),
                    "single_platform_news": len([a for a in aggregated if len(a["platforms"]) == 1])
                }
            }

        except MCPError as e:
            return {"success": False, "error": e.to_dict()}
        except Exception as e:
            return {"success": False, "error": {"code": "INTERNAL_ERROR", "message": str(e)}}

    def _aggregate_similar_news(
        self,
        news_list: List[Dict],
        threshold: float,
        include_url: bool
    ) -> List[Dict]:
        """
        å¯¹æ–°é—»åˆ—è¡¨è¿›è¡Œç›¸ä¼¼åº¦èšåˆ

        ä½¿ç”¨åŒå±‚è¿‡æ»¤ç­–ç•¥ï¼šå…ˆç”¨ Jaccard å¿«é€Ÿç²—ç­›ï¼Œå†ç”¨ SequenceMatcher ç²¾ç¡®è®¡ç®—

        Args:
            news_list: æ–°é—»åˆ—è¡¨
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            include_url: æ˜¯å¦åŒ…å«URL

        Returns:
            èšåˆåçš„æ–°é—»åˆ—è¡¨
        """
        if not news_list:
            return []

        # é¢„è®¡ç®—å­—ç¬¦é›†åˆç”¨äºå¿«é€Ÿè¿‡æ»¤
        prepared_news = []
        for news in news_list:
            char_set = set(news["title"])
            prepared_news.append({
                "data": news,
                "char_set": char_set,
                "set_len": len(char_set)
            })

        # æŒ‰æƒé‡æ’åº
        sorted_items = sorted(prepared_news, key=lambda x: x["data"].get("weight", 0), reverse=True)

        aggregated = []
        used_indices = set()
        PRE_FILTER_RATIO = 0.5  # ç²—ç­›é˜ˆå€¼ç³»æ•°

        for i, item in enumerate(sorted_items):
            if i in used_indices:
                continue

            news = item["data"]
            base_set = item["char_set"]
            base_len = item["set_len"]

            group = {
                "representative_title": news["title"],
                "platforms": [news["platform_name"]],
                "platform_ids": [news["platform"]],
                "dates": [news["date"]],
                "best_rank": news["rank"],
                "total_count": news["count"],
                "aggregate_weight": news.get("weight", 0),
                "sources": [{
                    "platform": news["platform_name"],
                    "rank": news["rank"],
                    "date": news["date"]
                }]
            }

            if include_url and news.get("url"):
                group["urls"] = [{
                    "platform": news["platform_name"],
                    "url": news.get("url", ""),
                    "mobileUrl": news.get("mobileUrl", "")
                }]

            used_indices.add(i)

            # æŸ¥æ‰¾ç›¸ä¼¼æ–°é—»
            for j in range(i + 1, len(sorted_items)):
                if j in used_indices:
                    continue

                compare_item = sorted_items[j]
                compare_set = compare_item["char_set"]
                compare_len = compare_item["set_len"]

                # å¿«é€Ÿç²—ç­›ï¼šé•¿åº¦æ£€æŸ¥
                if base_len == 0 or compare_len == 0:
                    continue

                # å¿«é€Ÿç²—ç­›ï¼šé•¿åº¦æ¯”ä¾‹æ£€æŸ¥
                if min(base_len, compare_len) / max(base_len, compare_len) < (threshold * PRE_FILTER_RATIO):
                    continue

                # å¿«é€Ÿç²—ç­›ï¼šJaccard ç›¸ä¼¼åº¦
                intersection = len(base_set & compare_set)
                union = len(base_set | compare_set)
                jaccard_sim = intersection / union if union > 0 else 0

                if jaccard_sim < (threshold * PRE_FILTER_RATIO):
                    continue

                # ç²¾ç¡®è®¡ç®—ï¼šSequenceMatcher
                other_news = compare_item["data"]
                real_similarity = self._calculate_similarity(news["title"], other_news["title"])

                if real_similarity >= threshold:
                    # åˆå¹¶åˆ°å½“å‰ç»„
                    if other_news["platform_name"] not in group["platforms"]:
                        group["platforms"].append(other_news["platform_name"])
                        group["platform_ids"].append(other_news["platform"])

                    if other_news["date"] not in group["dates"]:
                        group["dates"].append(other_news["date"])

                    group["best_rank"] = min(group["best_rank"], other_news["rank"])
                    group["total_count"] += other_news["count"]
                    group["aggregate_weight"] += other_news.get("weight", 0) * 0.5  # é¢å¤–æƒé‡

                    group["sources"].append({
                        "platform": other_news["platform_name"],
                        "rank": other_news["rank"],
                        "date": other_news["date"]
                    })

                    if include_url and other_news.get("url"):
                        if "urls" not in group:
                            group["urls"] = []
                        group["urls"].append({
                            "platform": other_news["platform_name"],
                            "url": other_news.get("url", ""),
                            "mobileUrl": other_news.get("mobileUrl", "")
                        })

                    used_indices.add(j)

            # æ·»åŠ èšåˆä¿¡æ¯
            group["platform_count"] = len(group["platforms"])
            group["is_cross_platform"] = len(group["platforms"]) > 1

            aggregated.append(group)

        return aggregated

    # ==================== æ—¶æœŸå¯¹æ¯”åˆ†æå·¥å…· ====================

    def compare_periods(
        self,
        period1: Union[Dict[str, str], str],
        period2: Union[Dict[str, str], str],
        topic: Optional[str] = None,
        compare_type: str = "overview",
        platforms: Optional[List[str]] = None,
        top_n: int = 10
    ) -> Dict:
        """
        æ—¶æœŸå¯¹æ¯”åˆ†æ - æ¯”è¾ƒä¸¤ä¸ªæ—¶é—´æ®µçš„æ–°é—»æ•°æ®

        æ”¯æŒå¤šç§å¯¹æ¯”ç»´åº¦ï¼šçƒ­åº¦å¯¹æ¯”ã€è¯é¢˜å˜åŒ–ã€å¹³å°æ´»è·ƒåº¦ç­‰ã€‚

        Args:
            period1: ç¬¬ä¸€ä¸ªæ—¶é—´æ®µ
                - {"start": "YYYY-MM-DD", "end": "YYYY-MM-DD"}: æ—¥æœŸèŒƒå›´
                - "today", "yesterday", "last_week", "last_month": é¢„è®¾å€¼
            period2: ç¬¬äºŒä¸ªæ—¶é—´æ®µï¼ˆæ ¼å¼åŒ period1ï¼‰
            topic: å¯é€‰çš„è¯é¢˜å…³é”®è¯ï¼ˆèšç„¦ç‰¹å®šè¯é¢˜çš„å¯¹æ¯”ï¼‰
            compare_type: å¯¹æ¯”ç±»å‹
                - "overview": æ€»ä½“æ¦‚è§ˆï¼ˆé»˜è®¤ï¼‰
                - "topic_shift": è¯é¢˜å˜åŒ–åˆ†æ
                - "platform_activity": å¹³å°æ´»è·ƒåº¦å¯¹æ¯”
            platforms: å¹³å°è¿‡æ»¤åˆ—è¡¨
            top_n: è¿”å› TOP N ç»“æœï¼Œé»˜è®¤10

        Returns:
            å¯¹æ¯”åˆ†æç»“æœå­—å…¸
        """
        try:
            # å‚æ•°éªŒè¯
            platforms = validate_platforms(platforms)
            top_n = validate_top_n(top_n, default=10)

            if compare_type not in ["overview", "topic_shift", "platform_activity"]:
                raise InvalidParameterError(
                    f"ä¸æ”¯æŒçš„å¯¹æ¯”ç±»å‹: {compare_type}",
                    suggestion="æ”¯æŒçš„ç±»å‹: overview, topic_shift, platform_activity"
                )

            # è§£ææ—¶é—´æ®µ
            date_range1 = self._parse_period(period1)
            date_range2 = self._parse_period(period2)

            if not date_range1 or not date_range2:
                raise InvalidParameterError(
                    "æ— æ•ˆçš„æ—¶é—´æ®µæ ¼å¼",
                    suggestion="ä½¿ç”¨ {'start': 'YYYY-MM-DD', 'end': 'YYYY-MM-DD'} æˆ–é¢„è®¾å€¼å¦‚ 'last_week'"
                )

            # æ”¶é›†ä¸¤ä¸ªæ—¶æœŸçš„æ•°æ®
            data1 = self._collect_period_data(date_range1, platforms, topic)
            data2 = self._collect_period_data(date_range2, platforms, topic)

            # æ ¹æ®å¯¹æ¯”ç±»å‹æ‰§è¡Œä¸åŒçš„åˆ†æ
            if compare_type == "overview":
                analysis_result = self._compare_overview(data1, data2, date_range1, date_range2, top_n)
            elif compare_type == "topic_shift":
                analysis_result = self._compare_topic_shift(data1, data2, date_range1, date_range2, top_n)
            else:  # platform_activity
                analysis_result = self._compare_platform_activity(data1, data2, date_range1, date_range2)

            result = {
                "success": True,
                "summary": {
                    "description": f"æ—¶æœŸå¯¹æ¯”åˆ†æï¼ˆ{compare_type}ï¼‰",
                    "compare_type": compare_type,
                    "periods": {
                        "period1": {
                            "start": date_range1[0].strftime("%Y-%m-%d"),
                            "end": date_range1[1].strftime("%Y-%m-%d")
                        },
                        "period2": {
                            "start": date_range2[0].strftime("%Y-%m-%d"),
                            "end": date_range2[1].strftime("%Y-%m-%d")
                        }
                    }
                },
                "data": analysis_result
            }

            if topic:
                result["summary"]["topic_filter"] = topic

            return result

        except MCPError as e:
            return {"success": False, "error": e.to_dict()}
        except Exception as e:
            return {"success": False, "error": {"code": "INTERNAL_ERROR", "message": str(e)}}

    def _parse_period(self, period: Union[Dict[str, str], str]) -> Optional[tuple]:
        """è§£ææ—¶é—´æ®µä¸ºæ—¥æœŸèŒƒå›´å…ƒç»„"""
        today = datetime.now()

        if isinstance(period, str):
            if period == "today":
                return (today, today)
            elif period == "yesterday":
                yesterday = today - timedelta(days=1)
                return (yesterday, yesterday)
            elif period == "last_week":
                return (today - timedelta(days=7), today - timedelta(days=1))
            elif period == "this_week":
                # æœ¬å‘¨ä¸€åˆ°ä»Šå¤©
                days_since_monday = today.weekday()
                monday = today - timedelta(days=days_since_monday)
                return (monday, today)
            elif period == "last_month":
                return (today - timedelta(days=30), today - timedelta(days=1))
            elif period == "this_month":
                first_of_month = today.replace(day=1)
                return (first_of_month, today)
            else:
                return None
        elif isinstance(period, dict):
            try:
                start = datetime.strptime(period["start"], "%Y-%m-%d")
                end = datetime.strptime(period["end"], "%Y-%m-%d")
                return (start, end)
            except (KeyError, ValueError):
                return None
        return None

    def _collect_period_data(
        self,
        date_range: tuple,
        platforms: Optional[List[str]],
        topic: Optional[str]
    ) -> Dict:
        """æ”¶é›†æŒ‡å®šæ—¶æœŸçš„æ–°é—»æ•°æ®"""
        start_date, end_date = date_range
        all_news = []
        all_keywords = Counter()
        platform_stats = Counter()

        current_date = start_date
        while current_date <= end_date:
            try:
                all_titles, id_to_name, _ = self.data_service.parser.read_all_titles_for_date(
                    date=current_date,
                    platform_ids=platforms
                )

                for platform_id, titles in all_titles.items():
                    platform_name = id_to_name.get(platform_id, platform_id)

                    for title, info in titles.items():
                        # å¦‚æœæŒ‡å®šäº†è¯é¢˜ï¼Œè¿‡æ»¤ä¸ç›¸å…³çš„æ–°é—»
                        if topic and topic.lower() not in title.lower():
                            continue

                        news_item = {
                            "title": title,
                            "platform": platform_id,
                            "platform_name": platform_name,
                            "date": current_date.strftime("%Y-%m-%d"),
                            "ranks": info.get("ranks", []),
                            "rank": info["ranks"][0] if info["ranks"] else 999
                        }
                        news_item["weight"] = calculate_news_weight(news_item)
                        all_news.append(news_item)

                        # ç»Ÿè®¡å¹³å°
                        platform_stats[platform_name] += 1

                        # æå–å…³é”®è¯
                        keywords = self._extract_keywords(title)
                        all_keywords.update(keywords)

            except DataNotFoundError:
                pass

            current_date += timedelta(days=1)

        return {
            "news": all_news,
            "news_count": len(all_news),
            "keywords": all_keywords,
            "platform_stats": platform_stats,
            "date_range": date_range
        }

    def _compare_overview(
        self,
        data1: Dict,
        data2: Dict,
        range1: tuple,
        range2: tuple,
        top_n: int
    ) -> Dict:
        """æ€»ä½“æ¦‚è§ˆå¯¹æ¯”"""
        # è®¡ç®—å˜åŒ–
        count_change = data2["news_count"] - data1["news_count"]
        count_change_pct = (count_change / data1["news_count"] * 100) if data1["news_count"] > 0 else 0

        # TOP å…³é”®è¯å¯¹æ¯”
        top_kw1 = [kw for kw, _ in data1["keywords"].most_common(top_n)]
        top_kw2 = [kw for kw, _ in data2["keywords"].most_common(top_n)]

        new_keywords = [kw for kw in top_kw2 if kw not in top_kw1]
        disappeared_keywords = [kw for kw in top_kw1 if kw not in top_kw2]
        persistent_keywords = [kw for kw in top_kw1 if kw in top_kw2]

        # TOP æ–°é—»å¯¹æ¯”
        top_news1 = sorted(data1["news"], key=lambda x: x.get("weight", 0), reverse=True)[:top_n]
        top_news2 = sorted(data2["news"], key=lambda x: x.get("weight", 0), reverse=True)[:top_n]

        return {
            "overview": {
                "period1_count": data1["news_count"],
                "period2_count": data2["news_count"],
                "count_change": count_change,
                "count_change_percent": f"{count_change_pct:+.1f}%"
            },
            "keyword_analysis": {
                "new_keywords": new_keywords[:5],
                "disappeared_keywords": disappeared_keywords[:5],
                "persistent_keywords": persistent_keywords[:5]
            },
            "top_news": {
                "period1": [{"title": n["title"], "platform": n["platform_name"]} for n in top_news1],
                "period2": [{"title": n["title"], "platform": n["platform_name"]} for n in top_news2]
            }
        }

    def _compare_topic_shift(
        self,
        data1: Dict,
        data2: Dict,
        range1: tuple,
        range2: tuple,
        top_n: int
    ) -> Dict:
        """è¯é¢˜å˜åŒ–åˆ†æ"""
        kw1 = data1["keywords"]
        kw2 = data2["keywords"]

        # è®¡ç®—çƒ­åº¦å˜åŒ–
        all_keywords = set(kw1.keys()) | set(kw2.keys())
        keyword_changes = []

        for kw in all_keywords:
            count1 = kw1.get(kw, 0)
            count2 = kw2.get(kw, 0)
            change = count2 - count1

            if count1 > 0:
                change_pct = (change / count1) * 100
            elif count2 > 0:
                change_pct = 100  # æ–°å‡ºç°
            else:
                change_pct = 0

            keyword_changes.append({
                "keyword": kw,
                "period1_count": count1,
                "period2_count": count2,
                "change": change,
                "change_percent": round(change_pct, 1)
            })

        # æŒ‰å˜åŒ–å¹…åº¦æ’åº
        rising = sorted([k for k in keyword_changes if k["change"] > 0],
                       key=lambda x: x["change"], reverse=True)[:top_n]
        falling = sorted([k for k in keyword_changes if k["change"] < 0],
                        key=lambda x: x["change"])[:top_n]
        new_topics = [k for k in keyword_changes if k["period1_count"] == 0 and k["period2_count"] > 0][:top_n]

        return {
            "rising_topics": rising,
            "falling_topics": falling,
            "new_topics": new_topics,
            "total_keywords": {
                "period1": len(kw1),
                "period2": len(kw2)
            }
        }

    def _compare_platform_activity(
        self,
        data1: Dict,
        data2: Dict,
        range1: tuple,
        range2: tuple
    ) -> Dict:
        """å¹³å°æ´»è·ƒåº¦å¯¹æ¯”"""
        ps1 = data1["platform_stats"]
        ps2 = data2["platform_stats"]

        all_platforms = set(ps1.keys()) | set(ps2.keys())
        platform_changes = []

        for platform in all_platforms:
            count1 = ps1.get(platform, 0)
            count2 = ps2.get(platform, 0)
            change = count2 - count1

            if count1 > 0:
                change_pct = (change / count1) * 100
            elif count2 > 0:
                change_pct = 100
            else:
                change_pct = 0

            platform_changes.append({
                "platform": platform,
                "period1_count": count1,
                "period2_count": count2,
                "change": change,
                "change_percent": round(change_pct, 1)
            })

        # æŒ‰å˜åŒ–æ’åº
        platform_changes.sort(key=lambda x: x["change"], reverse=True)

        return {
            "platform_comparison": platform_changes,
            "most_active_growth": platform_changes[0] if platform_changes else None,
            "least_active_growth": platform_changes[-1] if platform_changes else None,
            "total_activity": {
                "period1": sum(ps1.values()),
                "period2": sum(ps2.values())
            }
        }
